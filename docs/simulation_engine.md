Valorant Esports Management Game – Decision Simulation Design
Introduction & Design Philosophy
Summary: This document outlines a comprehensive design for simulating decision-making in a Valorant esports management game. The focus is on creating an engaging simulation game experience rather than pure realism. Realistic elements (like tactical teamplay and imperfect information) are used to enhance gameplay tension, variety, and replayability, ensuring each match feels dynamic and exciting for the player. Detailed Design: In this Valorant management sim, players take on the role of a coach or manager, observing and guiding a team of agents through matches. The simulation will model both macro-level decisions (team strategies, site executions, rotations) and micro-level decisions (individual movement, ability usage, dueling tactics). Every design choice prioritizes the gameplay experience: realism is included only insofar as it makes the simulation deeper and more suspenseful for the player. For example, simulating limited vision and audio cues creates tense situations and dramatic turnarounds, much like real Valorant where each round is a “psychological thrill ride” of mind games​

. Likewise, teamwork moments – such as one agent tagging an enemy with a recon dart so another can eliminate them through a wall – provide satisfying highlights that feel both realistic and rewarding​

. By balancing authenticity with fun, the system ensures that outcomes make sense (so the player can analyze and learn) while still allowing surprising plays that keep the game fresh. To cater to different audiences, each section of this design includes a high-level summary (for non-technical readers) and a deep dive into technical details (for engineers and designers). The ultimate goal is to support a deep, replayable, and satisfying simulation experience. The player should feel the thrill of outsmarting opponents with clever tactics, witness their team execute plans under pressure, and be able to replay and analyze matches endlessly without the simulation becoming predictable. In the following sections, we break down how the simulation models imperfect information, individual agent behavior, team strategies, the AI architecture behind these decisions, adaptation over time, internal data representations, and how all of this is presented to the player.
Imperfect Information & Perception Model
Summary: Valorant is a game of incomplete information – agents cannot see the whole map or all enemies at once. The simulation will replicate this imperfect information by giving each AI agent a limited perception based on line-of-sight and hearing. Agents make decisions with partial data: they see what’s in front of them, hear footsteps or gunshots nearby, and remember past observations. This creates realistic suspense and mind games, as agents (and teams) must infer enemy positions and intentions from limited cues, leading to believable mistakes, bluffs, and clutch plays. Detailed Design: At the heart of the simulation is a perception system that ensures no agent has omniscient knowledge of the battlefield. Each agent has a field of view representing what they can currently see (e.g. enemies in direct line-of-sight, or utility effects like smokes and flashes). Anything outside that view – behind walls or across the map – is unknown unless communicated by teammates or revealed through abilities. This models Valorant’s fog-of-war and angle clearing dynamics. In addition to vision, agents have a hearing system: they can detect nearby sound cues such as enemy footsteps, weapon fire, ability sounds, or the spike being planted/defused. These auditory signals give partial information – for example, an agent might hear gunfire on one site, guessing an engagement is happening there, or hear the distinctive sound of a Jett dash or Sova drone activation and infer an enemy agent’s action. Each agent maintains an internal knowledge state representing what they believe about the world. This includes: last seen positions of enemies (with timestamps), zones of the map that have been cleared or are unchecked, and any expected enemy behavior based on prior rounds (e.g. “the enemy often sends two players to B early”). This knowledge is inherently uncertain and decays over time – an enemy last seen 10 seconds ago may have moved. Agents use this state to make educated guesses. For instance, a defender might hold their aim on a corner where an attacker was last spotted even if they’re not visible anymore, or an attacking agent might decide to rotate because they heard multiple defenders rotating off a site. Crucially, the simulation incorporates team communication of info. When one agent gains important information (such as spotting the spike or hearing multiple footsteps), that info will be shared with teammates after a short reaction delay, mimicking callouts. We implement a simplified communication model: important knowledge (enemy spotted positions, spike location, etc.) is copied into a shared team knowledge base that all agents can use for decision-making. This means if one agent sees the enemy spike carrier at A, the whole team can react (e.g. rotating to A to defend). However, the simulation may also include occasional miscommunication or uncertainty for realism – for example, if two pieces of info conflict (one teammate thought they heard something at B while another sees enemies at A), agents might hesitate or split decisions. By enforcing imperfect information, we ensure the AI agents face the same strategic challenges as real players. They will sometimes make mistakes or brilliant reads under uncertainty, creating suspense. An agent might fall for a fake rotation, leaving a site weakly defended, or they might cleverly hold position because they suspect a lurk. This yields dramatic, varied rounds. As noted in a review, Valorant thrives on mind games where players must unravel opponents with limited clues​

 – our simulation will capture that essence. Each decision is taken with partial info: agents weigh the risks of what they don’t know. This not only injects tension and realism, but also improves gameplay by preventing the simulation from feeling deterministic. Even if the player sets the same strategy in two matches, the unfolding can differ because the AI’s information and assumptions differ each time, leading to high replayability.
Team Strategy and Macro Decision-Making
Summary: On the macro level, the simulation controls how the entire team operates as a unit each round. This includes selecting an overall strategy (for example, a slow default spread or a fast rush to a site), coordinating executions onto sites, mid-round rotations, and post-plant play. The team’s In-Game Leader (IGL) or strategy module will make calls based on the situation: they gather info, decide whether to commit to a site hit, fake and rotate, or save weapons. Each strategy is chosen to maximize the chance of winning the round given imperfect information and the current context (enemy tendencies, economy, etc.). The macro AI aims to emulate the tactical depth of real Valorant teams – spreading out for information, reacting to enemy setups, and coordinating utilities – all while keeping the gameplay fun and varied. Detailed Design: Round Initialization (Pre-round Planning): Before each round starts, the team AI (acting as an IGL) formulates a plan. This plan considers factors such as team composition, ultimate ability availability, and the enemy’s recent patterns. For example, if the enemy has been stacking one bombsite frequently, the AI might call a strategy to exploit that (like a fake on that site followed by a rotation). Common strategy archetypes the AI can choose from include:
Default Spread: A conservative strategy where players spread out across the map to control key areas and gather info without committing​
playbite.com
. In a default, each agent holds a different angle or zone, looking for any enemy aggression or an opening pick. This strategy “plays for picks” and keeps options open – if an attacker finds a kill or a weakly defended site, the team can quickly pivot there​
playbite.com
. Our simulation uses defaults to create slower, tactical rounds where information gathering is priority.
Rush/Fast Execute: A high-risk, high-reward strat where the whole team quickly hits a specific site together (often using pre-planned utility). The AI might call a rush when the team expects an economic advantage (e.g., enemies on an eco round) or wants to surprise the defense. Rushes in the sim lead to explosive early-round engagements and test the defenders’ reactive AI.
Split Push: A coordinated attack where the team splits into sub-groups to hit a site from multiple directions (e.g., three go main, two flank through mid). The macro AI ensures timing synchronization so both groups peek around the same time, overwhelming defenders.
Fake and Rotation: A deceptive plan where some teammates pressure or use utility on one site to draw rotations, while the spike quietly goes to the other site. For example, two agents might make noise and use smokes on A, then the team quickly rotates to hit B while the defense is pulled toward A. The AI decides to fake if it believes the enemy is quick to rotate or has shown vulnerability to misdirection in previous rounds.
The strategy selection is influenced by game context. The AI uses a simple heuristic or lookup to choose: e.g., on low economy rounds it might prefer a slow default or even a save/eco strat (avoiding costly fights), whereas with certain powerful ultimates ready, it might plan an execute to make use of them. It also avoids becoming too predictable by randomly mixing strategies to some degree, embodying the idea of conditioning – e.g., running a default several rounds to condition the enemy to play standard, then suddenly rushing to catch them off-guard (a classic mind game tactic). Mid-Round Calls and Adaptation: Once the round is live, the macro strategy isn’t static – the IGL logic can adjust the plan based on how the round unfolds. This is the mid-round decision-making that distinguishes good teams. The simulation continuously monitors key factors: Have we gotten a pick or suffered a loss? Did an agent spot the bombsite stack or hear a rotation? How much time is left? Based on this info, the team AI can call audibles such as:
Rotate to the opposite site: If the team encounters heavy resistance at the initial site (e.g., utility delay or multiple defenders spotted), the IGL might command a rotation to the other site, hoping it’s weaker. This decision is made under uncertainty; often the call to rotate or commit is what wins or loses rounds. Our AI will simulate this by evaluating the information gathered: for instance, if two agents pushing A see three enemies, it infers B might have fewer defenders and triggers a rotate.
Execute/Commit: Conversely, if the team secures an opening kill on a site or detects that defenders are out of position, the AI will commit to that site execute. It will signal all agents to converge and use remaining utility to take the site.
Reallocate Roles (Lurk/Flank): The macro AI might also instruct an agent to lurk (hold position to catch rotating enemies) or to flank. For example, in a default one agent holding mid might be told to continue lurking mid as the rest hit a site, creating pressure from behind.
Fall Back and Save: In dire situations (e.g., a 2v5 with little chance to recover the round), the AI can decide to have agents save their weapons for next round, especially on defense. This is a strategic macro decision to preserve economy, reflecting real esports scenarios.
These mid-round adjustments rely on partial info, making them exciting and fallible. The design emphasizes using whatever intel the team has: utilizing recon abilities and communication to guide choices. As one guide notes, attackers focus on gathering info and baiting out utility, while defenders aim to read the attackers and counter them​
theguide.gg
. The simulation mirrors this: an attacking team might pause after taking map control to bait out defensive smokes or ultimates (macro decision: stall and draw utility), whereas defenders might decide to push for information if they have no sight of the attackers by mid-round. Both sides are trying to make an informed move – adapt to the situation, coordinate with teammates, and execute the best possible plan​
theguide.gg
​
theguide.gg
. Post-Plant and End of Round: Once the spike is planted (or in the case of defenders, in a retake scenario), the macro AI governs how the team plays the post-plant. Attackers will typically take post-plant positions (e.g. crossfires, playing far from the spike with line-ups if they have agents like Sova or Brimstone). The simulation defines a few post-plant setups such as playing for line-ups (if agents have molotov or shock dart abilities to deny defuse from afar), playing site crossfire (if manpower is high to just defend the site), or playing off-site (leaving site and playing from safer positions, especially in man-disadvantage to play time). The AI chooses one based on numbers advantage and agent kit. For example, if Sova is alive with his ultimate or shock darts, the team might fall back and let him use utility to stop the defuse (a tactic real teams use)​

​

. Defenders, on the other hand, will decide whether to attempt a retake or save. Our simulation will consider factors like time remaining, manpower, and economy. A 3v3 retake with plenty of time will be attempted, with the AI coordinating utility usage (flashes, smokes) to retake angles. A 1v4 with little time might trigger a save call, where the remaining defender hides or retreats to preserve their weapon. This adds strategic depth – sometimes conceding a round is the best decision for long-term economy. Throughout these macro decisions, the guiding principle is to simulate a smart but human-like IGL. The AI will not always pick the perfect option (it doesn’t have full info); it will sometimes misjudge (e.g., rotating off a site just as the enemy commits there, or committing to a stack). These “wrong calls” are actually valuable for gameplay: they create dramatic tension and opportunities for the unexpected, which keeps matches interesting. The macro strategy system combined with imperfect info ensures that rounds can develop in a variety of ways, from slow tactical chess matches to chaotic scrambles, all under the control of a consistent strategic logic that the player can follow and influence (by setting overarching tactics or preferences in their management role). (High-Level Example:) To illustrate a macro decision sequence, imagine an attacking team running a default on Ascent. Two agents work A main slowly, one holds mid, and two hold B. Early in the round, the mid player (lurker) gets a pick on a defender pushing. The team AI immediately updates the plan: with a numbers advantage, it calls for an execute on A site (since they also heard a Sage wall on B, suggesting B is fortified). The A group uses a Sova recon and a flash to entry. They trade kills and get the spike down. Now in a 4v3 post-plant, the macro AI decides to play crossfires on site and A main. Meanwhile, the defender AI chooses to attempt the retake using their Killjoy lockdown ultimate. The attackers’ strategy module responds by ordering a temporary retreat out of the lockdown radius, waiting out the ult, then re-aggressing to stop the defuse. The attackers succeed, winning the round. In this scenario, we see the macro AI handle the initial strat (default), a mid-round shift (commit to A after pick), and post-plant coordination (dealing with a retake ult), showcasing a full round of strategic simulation.
Individual (Micro) Decision-Making and Agent Behavior
Summary: At the micro level, the simulation models how each individual agent acts moment-to-moment: moving, aiming, using abilities, and reacting to immediate threats. Each agent has a personal decision-making module that takes inputs (what the agent sees, hears, and the team’s current plan) and chooses actions like peeking an angle, falling back, using a flash ability, or taking a shot. Micro-level AI must capture the distinct behavior of different Valorant agents (e.g. how a duelist like Jett seeks entry duels vs. how a sentinel like Cypher holds a site). We provide fine-grained examples: for instance, how a Sova agent decides between using his Recon Bolt or Owl Drone in a given situation. The goal is to have each agent behave believably and effectively, contributing to the team’s strategy while also having individual flair and occasional mistakes, which creates exciting gunfights and highlights. Detailed Design: Core Micro AI Loop: Each agent continuously evaluates its situation through a perception-filtered world state (from the Imperfect Information system). On each decision tick (several times per second), the agent’s micro AI considers a set of possible actions, such as:
Move (to cover, forward, strafe, crouch, jump, etc.)
Turn/aim (adjust crosshair or check another angle)
Use an ability (throw a flash, activate a drone, place a smoke, etc.)
Fire weapon (if an enemy is in sights and a good shot is possible)
Communicate (shout a callout if something critical is observed)
These actions are decided using a combination of behavior trees and utility checks. We define behavior subtrees for common situations (e.g., engaging an enemy, clearing a corner, planting or defusing the spike). The agent’s role and current team plan set the context: for example, a designated entry fragger has a higher desire to push forward aggressively when executing a site, whereas a support agent will hang back and use utility. Movement and Positioning: Micro decision-making for movement involves how agents navigate the map and position themselves in combat. Agents will use cover and peeking tactics similar to players: jiggle peeking to gather info, wide swinging to take a duel if they have an advantage, or playing off-angles to surprise enemies. If an agent knows or suspects an enemy is around a corner, the AI will decide whether to slow peek (minimize exposure), flash and peek (if they have a flash ability available), or request help from a teammate for a double peek. These decisions depend on confidence and utility: e.g., a Breach agent might choose to use his flashbang ability through the wall before peeking a tough angle, whereas a Reyna with high confidence (maybe due to her Empress ultimate being active) might dry peek to take a duel, trusting her aim and dismiss ability to escape if needed. Positioning also involves holding angles vs. pushing. Agents consider the team strategy and their knowledge: a defender sentinel (like Killjoy) whose job is to anchor a site will hold a tight angle and wait for enemies, especially if her utility (alarm bot, turret) is set up nearby. In contrast, an aggressive duelist on defense might push for map control if the strategy calls for a flank. The micro AI looks at factors like: Do I have a escape ability available? (Jett’s dash, Reyna’s dismiss), Do I have teammate support?, What have I heard? (if the agent hears multiple enemies on the other side of a wall, a lone push might be suicide). Confidence plays a role here: if an agent is in a advantageous position (full health, good weapon, element of surprise), they’re more likely to attempt a push or wide swing. If low on health or outnumbered, they lean towards passive play, holding an angle or falling back to safety. Aiming and Shooting Decisions: When an enemy comes into view, the micro AI must decide how to engage. This involves target selection (if multiple enemies), whether to spray or tap fire, and whether to peek or wait. Our AI will simulate human-like firing behavior: if an enemy is caught unaware at close range, the agent may go for a quick spray transfer to take down multiple targets; if it’s a long-range encounter, the agent will tap or burst-fire for accuracy. Agents also know when not to shoot immediately – for example, if an agent is holding a crossfire and sees an enemy, they might wait a beat for the enemy to walk into the trap or for a teammate to also have line of fire, thereby securing a trade at worst. Another scenario is trigger discipline: an agent on a flank might see two enemies with backs turned; a smart AI may hold fire until they can line up both or ensure a more impactful timing (this could be an advanced behavior toggled by a “patience” parameter or personality trait in certain agents). Ability Usage: Each Valorant agent has unique abilities, and choosing when and how to use them is a critical part of micro decision-making. We give each ability a usage logic that looks at context cues:
Example – Sova (Initiator): Sova’s kit includes the Recon Bolt (E) and the Owl Drone (C), both for gathering information, but with differences. Our Sova AI will choose between them based on context. Recon Bolt is great for quick info on a location at range; it’s best used when Sova can safely dart an area before pushing or when expecting enemies hiding in a specific corner. It also doesn’t expose Sova to danger. Owl Drone, on the other hand, provides manual, detailed scouting but leaves Sova immobile while using it. The AI decision might go as follows: if the team is preparing to execute a site and needs to clear close corners or utility like Cypher traps, the Sova will prefer the Owl Drone (it can clear angles that a recon bolt can’t and even tag enemies)​

. The drone is chosen especially on attack when teammates can cover Sova, and it’s useful for scouting an Operator player holding a long angle safely​

. However, if time is short or Sova is alone, he’d rather fire a Recon Bolt to quickly gather info without the lengthy drone usage. The AI also tracks his ability cooldowns/charges: if he has only one Recon Bolt left for the next 40 seconds, he might save it for a more critical moment and use the drone now, or vice versa. This fine decision-making ensures Sova’s utility is neither wasted nor underutilized, contributing to realistic info-gathering play​

.
Example – Duelist (e.g. Jett): Jett’s micro AI handles her mobility and entry tools. If the team’s macro plan calls for an execute and Jett is the designated entry, her AI will use Tailwind (dash) to break onto site as soon as a path is opened (often after a flash or smoke is deployed by a teammate or herself). She might Updraft to an unexpected elevation to throw off defenders’ crosshair placement. If Jett is playing an Operator sniper role that round, her micro AI changes: she will hold an angle for a pick, and if she fires a shot (hit or miss), she immediately uses Tailwind to reposition to safety (a common Jett tactic). Jett’s AI also decides on her Cloudburst smokes usage – e.g., to block off a vision line while she dashes in, or to cover a retreat. These decisions are triggered by cues like “about to peek an exposed angle – use a smoke to cover one side” or “entering site – dash then smoke to create chaos and hard targets.”
Example – Controller (e.g. Omen): Omen’s micro decisions revolve around placing smokes at advantageous spots (often per the macro plan) and using Paranoia (flash) when an engagement is about to happen. If attacking, Omen’s AI will smoke common defender positions just as the team is about to push (timing it with the execute). If defending, Omen might reactively smoke a choke point when he hears multiple enemies approaching to delay them. His Shrouded Step (teleport) is used micro-wise to reposition in clutch moments (e.g., to get to a cheeky off-angle after smoking off the enemy’s view). The AI will only teleport if it sees an opportunity to gain an advantage (it checks that it’s out of enemy sight or in confusion of a fight). Poor use (teleporting in view of an enemy) is avoided except if desperation (which could occasionally happen to simulate errors).
Example – Sentinel (e.g. Cypher or Killjoy): A sentinel’s micro AI is about utility placement and area denial. Cypher will decide where to put Trapwires and his Spycam at round start (this could even be semi-randomized among a few good setups for unpredictability). During play, if his wire is tripped, he immediately reacts by pulling out his camera or peeking to capitalize. Killjoy’s AI will decide when to activate her Nanoswarm grenades if enemies step into a trapped area (with a slight human-like delay to not be too perfect). In post-plant defense, if Killjoy is alive and her turret or alarm-bot is still active on the other site, the AI might choose to keep those utilities unactivated as a form of flank watch, while focusing on the immediate fight.
Every agent also handles dueling and combat tactics in a way reflecting their role and kit. For instance, a Reyna AI, if she wins a duel, will immediately decide whether to use Devour (heal) or Dismiss (become intangible) – if there are other enemies nearby (detected or suspected), the AI favors Dismiss to escape; if she believes she’s safe after the kill, she’ll Devour to heal. A Phoenix AI will decide when to use his Curveball flashes around corners before peeking; if he has his ultimate Run It Back available, the AI might initiate a fight using it to scout or entry with less risk. Micro Coordination: While each agent’s micro AI primarily focuses on its own actions, it does consider teammates in the vicinity for local coordination. For example, if two agents are pushing together, they will attempt a swing-peek together: one AI might signal (through the team data or a simple nearby check) “ready to peek” and they synchronize their peek for a trade. If an agent is about to use a disruptive ability like Breach’s Fault Line (stun), he will signal his teammate AI to wait and peek right after the stun lands. This is implemented by setting short-term states like “preparing utility” that nearby allies can detect and respond to (holding position until the utility goes off). Such coordination is partly macro (planned executes) and partly emergent micro (agents recognizing opportunities to combo abilities in the moment). Human-like Imperfections: To ensure the simulation isn’t too robotic, we introduce variability in micro execution. Each agent has attributes that can cause different outcomes: aiming skill (inaccuracy or reaction time variation), pressure handling (maybe an agent’s performance slightly deteriorates in a 1vX clutch vs when with team). These can be tied to a “player profile” in the management game (e.g., an agent with high composure attribute won’t panic in clutch). This means sometimes an agent might whiff a shot or overheat (chase a kill too far) if their profile leans that way. As a result, even with the same strategy, micro-level events can diverge (one round your Sova hits all his scans and shocks, another he misses an arrow), contributing to replayability and a sense of individual personality. In summary, micro decision-making ensures each agent is a believable Valorant player analog. They use abilities smartly, move and shoot in plausible ways, and crucially, all their micro actions feed into the team’s macro objectives. A Sova using his recon dart to clear deep angles is doing so to gather info for the team​

, a Jett entry fragging creates space for the spike, and a Cypher holding a flank with a trap secures the team’s backside. By simulating this level of detail, the game produces rich narratives each round – from clutch 1v1 outplays to perfectly timed utility combos – driven by the micro AI decisions of each agent.
Hierarchical AI Design: Integrating Macro and Micro Decisions
Summary: To manage the complexity of five agents coordinating at macro and micro levels, the simulation uses a hierarchical AI design. This means we have distinct layers or modules for team-wide strategy and individual behavior, with a clear interface between them. One can imagine a top-level “Team AI (IGL)” that decides the overall plan and assigns roles, and the lower-level “Agent AIs” that carry out those roles and handle minute-to-minute actions. This hierarchy can be implemented as two AI models (one for macro, and one per agent for micro) working in tandem, or as a unified model with a hierarchical policy structure. We propose a hybrid two-tier system: a dedicated macro strategy module (the IGL brain) plus semi-autonomous agent modules. This choice is justified because it provides coordinated team behavior (from the top-down directives) while allowing individual flexibility and responsiveness (from bottom-up behaviors). It’s easier to design, debug, and tweak compared to a single monolithic AI, and creates more believable gameplay than completely independent agents. Detailed Design: In a hierarchical multi-agent system, a meta-agent or central planner can assign tasks to sub-agents to achieve a global goal​

. We apply this concept to Valorant rounds where the global goal is “win the round” (by planting/defending spike or eliminating the enemy). The team AI (meta-agent) breaks this into subgoals like “take control of bombsite A” or “play default for picks,” and assigns each agent a role in that plan (e.g., Agent1 and Agent2 execute A main, Agent3 lurks mid, etc.). The central planner effectively treats the individual agents as its “actions” or resources to deploy​

. For instance, calling a strat “execute A” is achieved by instructing agents to perform actions: use smokes here, entry there, hold flank, etc., which the individual AIs will interpret and carry out. The interface between macro and micro layers is carefully defined:
The macro AI does high-level decision-making at discrete points (pre-round, and at key junctures like after a kill or spike plant). Its outputs are strategic commands or objectives for agents. Examples: “Attackers: default spread formation,” “Rotate to B site now,” “Defenders: two push B lobby while others hold.”
Each agent’s micro AI receives these as part of its input (for example, an agent might have a state variable indicating the current team objective and that agent’s role in it). The micro AI then biases its decisions to fulfill that role. If the team objective is “execute A site,” a duelist agent’s micro AI knows it should take aggressive peeking actions toward A, whereas a lurker agent’s AI knows to hold position until team is ready. In technical terms, the macro AI could set something like agent.role = EntryA or agent.role = HoldMid and the micro behavior tree will choose different branches accordingly.
Micro AIs still operate continuously each tick, but they also listen for macro-level updates. We ensure the macro AI doesn’t micromanage every move (which could feel unnatural); instead it sets goals and let agents handle the how. This is akin to an IGL saying “go take A site” and the players figuring out the exact gunfights and utility timing on the fly, with practiced protocols.
We considered alternative designs:
A single unified model that takes the entire game state and outputs actions for all 10 agents. This was deemed impractical for design and not necessary for a game simulation. It would be like a giant brain controlling everyone, which is hard to make transparent or tunable. It might also remove the illusion of separate personalities or roles.
A fully decentralized model with no explicit team AI, where each agent just reacts on its own with perhaps some communication. While emergent coordination could arise (e.g., if all agents follow similar heuristics they might coincidentally execute a strat), it’s very difficult to ensure complex tactics (like well-timed site executes or fakes) consistently emerge. We want the sim to reliably produce sensible team strategies, not just individual skirmishes, so some central guidance is needed.
Thus, the hierarchical (hybrid) approach is ideal. It matches how real teams operate (with an IGL and individual players) and aligns with known AI architectures for multi-agent systems, where a top layer assigns sub-tasks to agents​

​

. In our design, the team strategy module can be thought of as the “coach/IGL AI” which might be implemented via a behavior tree or even a simple state machine that switches strategies based on conditions. The individual agents are each running their own behavior trees/utility systems that know how to execute orders. For example, at round start, the macro AI chooses “default.” It assigns each agent a default position. Agent A’s micro AI gets the command to hold A lobby, Agent B to hold B lobby, etc. They then move to those positions and wait. During the round, Agent A picks off an enemy. The macro AI then decides “group and hit A now.” It sends a new command: all agents converge on A. The micro AIs drop their default behavior and switch to execute mode, moving toward A, using utility as per their training for site take. Agent C’s micro AI might specifically get “use smoke Heaven” as part of the execute instructions (if Agent C is the controller), which it then carries out by choosing the smoke ability at the right location. In this manner, the macro AI sets the what/where, and micro AI decides the how. Justification: This separation of concerns makes the system manageable:
The macro layer can be designed and tweaked by gameplay designers to ensure the AI uses a variety of tactics and matches the strategic depth desired. We can easily adjust how often the AI fakes, or how it prioritizes bomb sites, without touching micro behaviors.
The micro layer can be developed by AI engineers focusing on combat behavior, cover usage, ability logic, etc. They ensure each agent is individually strong and fun to watch. They don’t have to worry about larger strategy in their code.
The emergent result is coordinated but flexible. If something unexpected happens that the macro AI didn’t explicitly plan (say an agent gets a surprise kill on the other side of the map), the micro AI can capitalize on it immediately (maybe that agent goes to site alone and lurks deep). The macro AI will catch up on the next decision tick and possibly adjust the plan, but it doesn’t micromanage that instantaneous reaction. This mimics players deviating from a plan in the moment if they see an opportunity.
One challenge in hierarchical systems is preventing them from feeling too rigid. We address this by giving micro agents a degree of autonomy and the ability to override or request changes to the plan. For instance, if the macro plan says “hold positions” but an agent suddenly sees the spike on their side, that agent’s micro AI can decide to pursue the spike (a high-value action) even if it means breaking the formation. The agent would also signal the macro AI (via the shared knowledge) of the spike, so the macro layer can update the strategy accordingly. Essentially, it’s a two-way interaction: macro directs micro, but micro feeds back critical events. This is where our design leans toward a hybrid model rather than strictly top-down. We allow the “free will” of agents to an extent, ensuring they don’t suicidally follow orders that no longer make sense (like running into a stack because they were told to go A at round start even if it’s clearly a bad idea now). Technically, we might implement this via priority behaviors in the micro AI that supersede orders if survival or crucial objectives demand it. E.g., an “if I am about to die, retreat” behavior might override a push order, or “if spike is down in front of me, guard it” might override a previous plan to rotate elsewhere. The macro AI could then re-evaluate given this new development. By using this hierarchical design, we achieve a balance between coordination and individual realism. It’s analogous to how a coach gives a game plan but players still make split-second choices. This leads to a simulation that is both strategically rich (thanks to the top-level planning) and dramatically realistic (thanks to bottom-level improvisation). And from a development standpoint, it is modular and clear: we can refine team strategy logic and individual behavior logic separately, ensuring the final product meets the gameplay vision.
Evolving Strategies and Adaptation
Summary: A static AI would make a sim game dull quickly, so our design emphasizes evolution and adaptation over the course of a match (and even between matches). The AI-controlled teams will learn from what’s happening and adjust their decisions in later rounds – effectively engaging in the same kind of mind games and conditioning that real esports teams do. Early rounds might be used to probe the opponent; if a pattern is detected (e.g. the enemy always stacks site A or a certain player always flanks), the AI will change tactics to counter it in future rounds. Likewise, the AI might intentionally switch up its own strategy to avoid being predictable. This dynamic adjustment ensures each match feels like a living, developing battle of wits, and it challenges the player to also adapt their management approach. Detailed Design: Round-to-Round Memory: We implement a memory system for each team that stores key information from previous rounds. This could include:
Frequency of opponents appearing at each location (e.g., “Enemy tends to push mid in 3 of the last 5 rounds”).
Strategies the opponent team used (e.g., “They have executed B site fast twice in a row” or “they play default when on low buy”).
Outcomes of our own strategies (e.g., “Our A executes succeeded 2 out of 2 times” or “Our slow default failed when enemy pushed us aggressively”).
Using this memory, the macro AI can apply adaptive logic at the start of each new round (and sometimes mid-round). For instance, if the enemy has a star player who has been consistently getting first kills on one site, the AI might decide to avoid that player next round or allocate additional utility to counter them (like double-flashing that angle or using a different approach). If our team’s attempts to attack B have been constantly shut down by a Killjoy setup, the AI might shift to prioritize A site hits or execute with an extra step (like use Sova drone to clear Killjoy's setup before committing). One straightforward adaptation mechanism is weighting the strategy selection. Initially, strategies might be weighted equally or according to a pre-match plan. As rounds progress, the AI will increase the weight of successful strats and decrease the weight of failed ones in similar contexts. For example, if fake rotations have worked multiple times (causing the enemy to over-rotate), the AI might call another fake to exploit this tendency. Conversely, if the enemy isn’t falling for fakes at all (they hold their positions), the AI will stop wasting time on elaborate fakes and commit more straight executes. Conditioning and Mind Games: The AI is also designed to incorporate intentional variability to prevent the opponent (or the human player observing) from easily predicting it. This can be seen as the AI trying to “condition” the enemy. For example, the attacking AI might run three slow defaults in a row. Seeing this, the defending AI may start pushing for information earlier in rounds to counter the slow play. If our attacking AI detects this adjustment (perhaps noticing “the last two rounds, defenders pushed out by mid-round”), it can then change pace dramatically: e.g., call a fast rush in the next round to catch those pushing defenders off guard. This is a classic mind game: lull the opponent into expecting one style, then flip the script. Our simulation can schedule such switches either through a bit of randomness or rules like “don’t repeat the same strat more than X times in a half” or “if opponent starts countering, do the unexpected.” On the defensive side, conditioning might involve allowing a certain part of the map to be taken without resistance in early rounds (so attackers think it’s always clear), then suddenly contesting it with a push or trap. For instance, defenders let attackers take mid control on Haven for several rounds, then in a crucial round they double-push mid at the start and get a surprising double kill. The defensive AI would decide this by noticing the attackers always go mid and maybe becoming confident that a well-timed aggression could work. We also simulate in-round adaptation often referred to as conditioning during a single round: e.g., an agent jiggle-peeks a corner twice without shooting to bait out an enemy’s shot, conditioning them to think it’s safe or to get their crosshair off, then on the third peek the agent wide swings and attacks. These micro mind games are part of the agents’ behavior logic (like varying timing on peeks, not always reacting the same way to a stimulus). Adaptive Difficulty and Fairness: For gameplay purposes, we can adjust how quickly and how well the AI adapts. A too-perfect adaptation could feel unfair or too difficult, so for a balanced experience we might simulate a certain delay or learning curve. For example, the AI might need a few rounds of data to confidently change its strategy – it won’t instantly counter after one round (unless it’s something very obvious like seeing a specific ultimate used, etc.). This gives the player, who might be adjusting their own team or strategy, a chance to react as well. Ideally, adaptation should feel like a chess match: both sides make moves and counter-moves over time. Long-Term Adaptation: In a longer context (like a best-of series or season), the AI could also carry over knowledge. If the player’s team frequently uses a particular strategy, AI opponents might come prepared in future matches. However, implementation of long-term memory can be complex, so initially we focus on within-match adaptation (round-to-round). Example Scenario: Suppose the player’s team (controlled by our AI) has noticed that the opponents have a weak B site defense on Icebox (perhaps because the enemy sentinel always plays A). The first few rounds, whenever the team went B, they won. The AI increases the weighting to go B more often. However, a smart opponent AI might shift their sentinel to B or start triple-stacking it after halftime. If our AI now encounters heavy resistance at B (unexpected utility, more defenders), it should adapt by not stubbornly hitting B. It might try a couple more times to confirm, then start exploring A or mid. Essentially, both sides are in a loop of adjust, test, re-adjust. The adaptation system is closely tied with the knowledge and state representation (discussed in the next section): the AI relies on stored data about what happened. For instance, a simple data point like “how many defenders were present when we hit site X” can inform whether the site was stacked or weak. The AI can glean patterns like “when we go A with heavy noise early, the enemy rotates quickly” – thus next time, use that to fake A. These are coded as rules or patterns the AI can detect (we can script a handful of common patterns to watch for, such as opponent fast rotates, opponent save tendencies, specific agent positioning). By building in these adaptive behaviors, we aim to avoid repetitive AI that does the same thing every round. Instead, the match will feel interactive and evolving. The human player will notice, for example, “the enemy AI started pushing us because we were playing too slow – I need to adjust our style,” which is exactly the kind of decision a real coach would make. This loop greatly enhances replayability: even if you replay the same matchup, slight differences in early rounds could lead the teams to adapt along a different path, creating a new experience.
Knowledge Representation and Data Structures
Summary: Under the hood, the AI needs to keep track of a lot of information – what each agent knows, what the team as a whole knows, how confident they are in that knowledge, and resources like abilities or ammo. We design clear data structures to maintain the game state as perceived by the AI. Each agent will have a knowledge base containing things like enemy positions (last seen/heard), status of objectives (spike location), and personal status (health, ammo, abilities available). The team will have a shared memory for communicated info. We also include variables for confidence (how certain an agent or team is about a belief or strategy) and track all cooldowns and ability charges. These structures ensure decisions are based on relevant, up-to-date info and allow the AI to update its strategy logically. They also make it easier to surface this info to the player in logs or replays. Detailed Design: The data representation can be thought of in layers:
World State (Real): The actual game state (true positions of all players, who is alive, etc.) – in simulation this is known to the game engine but not directly to AIs.
Agent Knowledge State (Perceived): For each AI agent, we maintain an object that represents what that agent believes about the world. Key components:
Visible Enemies: A list or set of enemies currently in line-of-sight. If an enemy is visible, the agent knows their exact position (and likely which agent it is, since they see the character). This list updates each tick.
Heard Enemies: A record of recent sound events. For example, heardEvent: {type: footsteps, location: B Garage, time: 45s} meaning at 0:45 round time the agent heard footsteps at B Garage. We might not know how many enemies, but we mark that something was heard. Another example: heardEvent: {type: gunshot, agent: Phoenix, location: A Short, time: 30s} if a specific agent’s ability voice line or gun sound was recognized.
Last Seen Positions: A dictionary of enemy agent -> last known position & time. If enemy Sage was seen at A rafters at 40s, this stays in memory until overwritten. The agent’s AI uses this to pre-aim or anticipate, albeit with decreasing confidence as time passes.
Inferred Positions: For enemies not seen or heard, the agent can have an inferred probability map or simple assumption of where they might be. For simplicity, we might not maintain a full probability distribution, but we tag areas as “uncleared” or “potential enemy here”. For example, if no one has checked Site B, an agent will assume “Site B could have enemies” and proceed with caution there. This can be a boolean flag per map region or a more granular grid.
Ally Positions and Status: The agent knows where allies are (from communications) and their general status (alive, health if communicated, or at least if they need help). This is mostly fully observable to all on the same team, except maybe slight delays.
Objectives & Items: If the spike is down (dropped or planted), if sighted, that position is recorded. If an ultimate orb was taken and noticed, that info could be stored (though not crucial).
Utility and Environment: Knowledge of active smokes, walls, etc. For instance, an agent might note “Omen smoke at mid will dissipate in X seconds” if they saw it cast (though agents typically know approximate durations of common smokes by game design knowledge). They also track their own utility timers (e.g., Phoenix wall flame lasts X sec).
Agent’s Own State: Health, ammo, remaining bullets in clip, currently equipped weapon, abilities available (and their charges or cooldowns), ultimate charge. This isn’t “knowledge” per se but part of state informing decisions (e.g., if low ammo, maybe don’t peek multiple enemies; if you have ultimate ready, you might use it in a clutch).
Confidence/Mental State: We give each agent a parameter for confidence or risk-appetite. This could be influenced by factors like “are we winning or losing the round,” “is the agent alone,” or even a preset personality trait. High confidence might push the agent to take a duel; low confidence might make them play time or hide. This state can be updated during the round (e.g., if three allies die quickly, a remaining agent’s confidence to retake might drop, influencing them to save).
Team Knowledge Base: This is akin to a blackboard system where agents publish information that everyone on the team can use. Entries in team knowledge include:
Enemy spotted: When an agent sees an enemy, an entry enemy X spotted at location L at time T is posted. Other agents incorporate this into their own knowledge (perhaps with a slight time stamp difference to simulate communication delay).
Spike status: If spike is seen or dropped or planted.
Strategy info: The current team strategy or call can also be part of shared state, e.g., current_call = "rotate to B". This way all agents know what the IGL ordered.
Warnings: If an agent is holding one side and has to leave it, they might post “flank not covered” so others know a gap exists.
Enemy tendencies (over the match): A compiled summary like “enemy often 3 stack A” could be placed here by the macro AI, so all agents sort of have a sense (this might affect how aggressively a lurker lurks, for instance).
The data structures could be implemented as simple classes or structs in code. For example:
python
Copy
Edit
class AgentKnowledge:
    visible_enemies = []  # list of EnemyInfo
    last_seen = {enemy_id: (position, time)}
    heard_noises = [NoiseEvent(...), ...]
    ally_info = {ally_id: AllyInfo(position, ...) }
    spike_info = SpikeInfo(location, status)
    abilities_status = {ability_name: cooldown_or_charges}
    health = 100
    confidence = 1.0  # baseline 1.0, may range 0-2 for low/high confidence
    # ... etc.
And a TeamKnowledge shared object:
python
Copy
Edit
class TeamKnowledge:
    enemy_spotted = {enemy_id: (position, time, reporter_agent)}
    spike_known_location = (position, time, seen_by) or None
    current_strategy = StrategyCall(name="Execute B", issued_by="IGL", time=...)
    # maybe a log of recent events for debug/analysis
Agents would update these structures as the game progresses, and consult them when making decisions. Cooldown and Resource Tracking: Each agent tracks its own cooldowns in abilities_status as noted, and possibly we maintain an estimated cooldown tracking for enemy abilities as well. For example, if our agent sees enemy Raze use her grenade (Paint Shells), they know Raze won’t have that for the rest of the round (since it’s one charge per round). Or if an enemy Reyna gets a kill, our AI might mark that Reyna could now be invisible (Dismiss) or overhealed, etc. While the AI doesn’t “cheat” by knowing exact enemy cooldowns without reason, it does know the rules of the game. A sophisticated extension could have the AI infer economy and ult status of opponents: e.g., if we saw enemy use an ultimate last round, we know it’s gone; if we killed an enemy who had their ultimate (the sim can track ult points), that might reset. These are more advanced and can be simplified if needed. Team Communication Implementation: When an agent posts to TeamKnowledge (like an enemy spotted), that info can immediately be available to others. However, to mimic imperfect comms, we might introduce a slight delay or even a chance an agent doesn’t communicate a minor detail (to avoid omniscience). We might not model miscommunication in detail (since it can frustrate if AI acts on wrong info arbitrarily), but lack of info is already there from imperfect perception. The key is all agents largely share what they do perceive. Confidence and Morale: The concept of team confidence can also be tracked. For instance, if a team is up 5v2 in a round, their confidence goes up (perhaps making them a bit more aggressive to close out the round); if they are down 2v5, confidence plummets (more likely to save or play very defensively). We can implement this as a simple counter: confidence = (our_alive^2 / enemy_alive^2) or some function of advantage. It then influences agent behavior through their risk thresholds. This adds variability: a team that’s feeling dominant might go for a bold play; a team on the back foot turtle up. Over a match, if a team loses many rounds in a row, their overall strategy might shift to more desperate plays (or the opposite, cautious eco saves) depending on design, which can simulate a form of momentum or morale that the player can try to manage (maybe through timeouts or pep talks, if those features exist). Data for Post-Match Analysis: We also log key decisions and observations in these structures for later use in replays or logs. For example, we could store each agent’s decision history like “At time 35s, Agent used ability X because heard noise Y” which can later be presented to the player as part of a match log or debug info. These data structures thus serve dual purpose: real-time decision-making and storing a narrative of what happened and why. In summary, robust data structures for knowledge and state are the backbone that enables all of the above systems (imperfect info, macro strategy, micro action, adaptation) to function coherently. They ensure that an agent’s actions are grounded in what that agent knows at that time – so when an AI makes a smart play or a mistake, it’s traceable to the information it had or lacked, just like a real player. By structuring the data well, we also make the simulation explainable: every decision can be tied to a piece of knowledge or lack thereof, which is great for debugging and for building player-facing tools to understand the simulation.
Player Visibility and Feedback
Summary: To make the simulation satisfying as a management game, the player needs ways to observe and understand the matches. We will provide rich feedback through tools like match logs, replays, heatmaps, and decision breakdowns. The idea is to expose the depth of the simulation in an accessible manner: the player can watch replays of rounds to see strategies unfold, view diagrams or heatmaps that show where engagements happened, and read summaries that explain key decisions (e.g., “Team decided to rotate after spotting two operators on Site A”). By making the AI’s logic transparent, the game not only entertains but also lets the player learn and refine their strategies. This feedback turns each match into a learning experience and a compelling story, enhancing long-term engagement. Detailed Design: Match Viewer/Replays: The primary way a player will experience the simulation is by watching the matches (either live or replay). We will implement a top-down 2D replay viewer (or even a 3D if feasible) where the player can see icons for each agent moving on the minimap, with an option to highlight vision cones and heard sound radii to illustrate the imperfect information in play. This gives a visualization of how the round played out. The replay can have play/pause/rewind and possibly a step-through of key phases (e.g., freeze at the moment of execute to show setup). We might also allow the player to click on an agent during replay to see their point-of-view (what they saw, perhaps even simulate their screen) to really emphasize how the AI was making decisions with partial info. This feature directly leverages the agent knowledge state: we can display what an agent knew at any given time (like drawing question marks where enemies were suspected vs red markers where enemies were seen). For each round, a round summary log will be generated (either in text form or a timeline UI). This summary would include important events and decisions, for example:
Round start: chosen strategy (e.g., “Round 5: Attackers execute A, with one lurker mid.”).
Key events: “0:45 – Sova uses Recon Bolt at A Main (spots 1 enemy)​

. Seeing only one, attackers suspect site is weak. 0:40 – Team calls push onto A.”, “0:30 – Two attackers entry, Jett gets first kill. Omen smokes off Heaven.”, “0:25 – Defenders kill the lurker mid, call for rotate.”, “0:20 – Spike planted. 3v3 situation.”, “0:10 – Defender Killjoy ult initiated for retake. Attackers fall back out of site.”, “0:00 – Time runs out after successful defense of spike. Attackers win.”
Outcome: “Win/Loss, surviving players, economy impact.”
Alongside this, we could include a bit of AI reasoning in the log, written in simple language. For example: “Because Sova’s dart revealed only one defender on A, the team decided to commit to an A hit, expecting at most one more defender there​
theguide.gg
. After planting, noticing they were outnumbered, they played passively for time.” These snippets tie the events to decisions and the info behind them, giving the player insight into the AI’s thought process. Heatmaps and Statistics: After a match (or during halftime), the player can access analytics like heatmaps. A heatmap might show where kills happened on the map or where each team tended to engage the most. This can reveal patterns (e.g., “most fights happened around mid – maybe focus on that area” or “the enemy never went C long at all”). We can generate heatmaps by accumulating position data from the simulation rounds. Similarly, we can show the paths each agent took each round or on average – useful for spotting if a player is consistently taking the same route (and thus might be predicted in the future). We can also present performance stats: headshot percentages of agents, utility usage counts (e.g., how many flashes were effective), average time to first kill per team, etc. From the decision-making perspective, interesting stats might be “successful executes vs failed executes” or “times we correctly predicted stack vs got caught by stack.” These could be translated into coach-like metrics (perhaps a measure of how well the AI is adapting or how often the first kill is by your team vs enemy). Interactive Decision Trees/Flowcharts: For players who want a deep dive, we could include an interactive decision flow diagram for critical rounds. For example, clicking on a lost round might show a flowchart: Planned Strategy -> Early Round -> Mid Round decision point -> Outcome. It might say:
Planned: Default split.
Early Round: No contact at B, moderate resistance at A.
Mid Round Decision: Rotate to B (because no contact B and Killjoy lurk reported site clear).
Late Round: Walk into Killjoy utility stack at B -> round lost.
Analysis: The decision to rotate was based on false assumption (Killjoy’s lurk missed one enemy who was hiding). Perhaps staying A would have been better. This can be shown textually or with nodes, and ideally linked to replay (click a node to jump to that moment in replay).
While implementing a full decision tree UI might be complex, at minimum a textual commentary or coach’s report could be generated. E.g., “The team’s mid-round call to rotate to B backfired, as the enemy had actually stacked B. This happened likely because the lurker didn’t clear cubby. Improvement: ensure thorough clearing or use utility to confirm site empties.” Such analysis might even be a gameplay element (like the game’s virtual analyst giving tips). We will make sure to keep these explanations digestible. Non-technical players can stick to high-level summaries and highlight replays (like viewing just kills and final 1vX situations), while more engaged players can read the detailed logs and analysis. Crucially, because every piece of logic (macro calls, micro actions) is recorded and tied to known factors, we can always provide a reason for what happened. This prevents the player from feeling the sim is “random” – instead they can see why an agent behaved a certain way, even if it was a mistake (e.g., “Agent thought area was clear due to outdated info”). This transparency builds trust in the simulation’s integrity and gives the player the tools to strategize better (just like reviewing demo footage in real esports). UI Implementation: These features would be accessed through the game’s interface around the match. For example, after a match, a “Review Match” button leads to a dashboard with:
A replay viewer (with play/pause, speed controls, and possibly an overhead tactical map view).
A “Match Log” tab listing rounds with expandable details and key events.
A “Analytics” tab for heatmaps and stats.
If applicable, a “Decision Analysis” tab for narrative insight.
During a match, if it’s simulated live, the player might see a simplified commentary like, “Round 3: Your team defaults. (10 seconds later) Jett finds a pick mid. IGL calls to rotate to A.” This textual play-by-play can make watching live more engaging and understandable. This is similar to how spectators or commentators break down a round. We can generate these from the same data structures. All these exposures serve the end goal: making the player feel connected to the depth of the simulation. Instead of just seeing a win or loss, they get the story and reasoning. This increases the satisfaction of victories (knowing how your strategy succeeded) and softens defeats (learning what went wrong and how to improve). It also encourages the player to keep playing (replayability) since they can always dive back in to analyze a classic match or experiment with a new strategy and then study its effect through the provided tools.
Conclusion
In conclusion, this design document presents a robust framework for a Valorant esports management game’s decision-making simulation. We have detailed how the system will handle everything from high-level team tactics down to split-second individual actions, all under the guiding principle of creating an engaging simulation rather than a dry replica of reality. Realism is leveraged where it enhances gameplay – such as the suspense of imperfect information or the authenticity of varied agent abilities – but always with an eye toward fun and depth. The macro and micro layers work in harmony through a hierarchical AI design, ensuring teams coordinate effectively while agents retain individual personality and reactivity​

. The simulation evolves with each round, featuring adaptation and mind games that make every match unpredictable and challenging​
theguide.gg
. We’ve also specified the data structures that keep the AI “thinking” logically and consistently, as well as how the rich inner workings of the simulation will be presented to the player through replays, logs, and analytics. By implementing these systems, the game will offer a deeply replayable experience. No two matches will play out exactly the same way, and players will find themselves engrossed in the strategic narratives that develop. They can marvel at a perfectly executed site take, sweat during a 1v1 clutch, and then afterwards break down exactly how it all happened. This design aims to satisfy both the armchair strategist and the curious engineer in every player – providing high-level excitement and low-level insight. Ultimately, the measure of success will be a simulation that produces believable Valorant esports action, full of drama and smart plays, that players can watch and manage for hours on end, always discovering new nuances. With this design, we lay the groundwork for a management game that feels as intense and rewarding as watching a real Valorant tournament, with the added satisfaction of knowing you orchestrated the victory.​