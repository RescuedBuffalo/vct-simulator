AI Player Modeling for a Valorant-Style Esports Game

Design Philosophy and Goals

Our goal is to develop a realistic, high-performance AI player for a tactical 5v5 shooter (Valorant-style) using deep reinforcement learning. The AI will control individual in-game players under the same constraints as human players. Key design principles include:
	•	Partial-Information Play: The AI operates with limited knowledge (only what a real player would perceive) to ensure fairness and realism. This means the agent must rely on its own perspective, team communications, and memory of past events, modeling the game as a partially observable environment (POMDP).
	•	Self-Play Reinforcement Learning: We will train the AI via self-play, having AI agents scrimmage against each other in countless simulated matches. Self-play at scale has proven effective in achieving human-level or superhuman performance in complex multi-agent games￼. By running thousands of parallel matches and having agents learn from wins/losses, we expect emergent strategies and teamwork to develop.
	•	Flexible, Personality-Driven Behavior: The AI’s behavior will be parameterized by “player personality” settings. We design a reward function with tunable weights that emphasize different objectives (e.g. kills vs. survival). By adjusting these weights or inputs, the same underlying AI model can exhibit distinct play styles – a “kill-hungry” duelist versus a “team-first” support player, for example. This flexibility allows us to populate our game with varied AI personalities.
	•	Polished and Human-Like Performance: Through extensive training and careful reward shaping, the AI should demonstrate deep tactical understanding, mechanical skill, and believable decision-making. The aim is to exceed typical game bots, reaching a level where the AI can cooperate with human teammates and challenge skilled players. Prior research shows that with the right training regime, agents can learn human-like navigation, teamwork, and tactics in FPS games￼￼.
	•	Maintainability and Scalability: The system should be designed such that we can continually improve the AI over time (by further training or fine-tuning) and scale the difficulty or style as needed. We will favor a unified model approach (with shared parameters) over a collection of disparate models, to ease updates and ensure consistency across AI-controlled players.

By adhering to these principles, we plan to build an AI that is not only strong and effective, but also adaptable to different roles, skill levels, and personalities – a major differentiator for our game.

Observation Space (State Inputs)

Each AI-controlled player will receive an observation input that represents what a real player would perceive at a given moment. This observation encapsulates the partial information available to that agent. The observation space includes multiple categories of features:
	•	Self State: The agent’s own status, such as current health, armor, ammo, current weapon, location (coordinates on the map), facing direction, and whether it’s performing certain actions (e.g. is planting or defusing the objective). It also includes the agent’s role/character and any unique ability status (cooldowns, remaining charges of abilities, ultimate charge, etc.).
	•	Team State (Known Ally Information): High-level info about teammates that a player would reasonably know. For example: teammate positions or at least their last known positions (if communicated), their status (alive or dead, health if communicated), and who has the objective (spike carrier). We maintain a communication blackboard where allies share info like enemy sightings or map callouts. The observation may include entries from this comms buffer (e.g. “ally saw enemy at B site”) and known strategic info (which areas are cleared or under control). This simulates in-game communication and map awareness.
	•	Enemy Information (Partial): The agent does not have omniscient knowledge of enemies. Instead, it gets a list of any enemies currently visible in its field of view (with their relative positions or distance). If an enemy was seen recently or an ally called them out, that last known position could be provided (with some time decay). Also, auditory cues: if the player hears footsteps or gunshots, the observation includes a “sound event” with an approximate location or direction￼￼. Essentially, we feed the agent the same cues a human would rely on: visual contacts and sound cues, each with appropriate uncertainty.
	•	Objective and Map State: Information about the objective (spike) status: e.g. whether the spike is planted and time until detonation, or whether it’s been dropped and its known location￼. Also include the round timer, and perhaps a simplified map layout encoding. The map could be represented to the AI in terms of key regions or zones (e.g. bomb sites, common routes) with flags indicating control or presence of allies/enemies as known. For instance, if two teammates are holding Site A and have not seen anyone, the AI might infer that area is likely safe; such notions can be captured via features like “ally control percentage of area” or through the allies’ positions and comms.
	•	Inventory and Economy: At the start of rounds (buy phase), the observation will include the player’s current credits and the available shop options. It will know what it already has (weapon, shield, abilities) and what teammates have bought if that’s communicated. This allows the AI to make purchasing decisions. During rounds, ammo counts or remaining utility charges are part of self state.
	•	History (Memory): Because the environment is partially observable and temporal, the agent benefits from remembering past observations. We won’t explicitly give a stacked history of observations, but the model’s internal state (recurrent memory) will learn to carry relevant info (e.g. “I heard footsteps in mid 5 seconds ago” or “Spike was last seen on the ground at site B”). This is handled by the model architecture (using an LSTM or similar mechanism to provide memory of previous states).

All these features together form the input to the AI’s neural network at each decision step. The design ensures the AI only knows what it should: anything outside its sensory range or not communicated by allies is absent, forcing the AI to infer and anticipate like a human player. This observation design sets up the challenge for our learning algorithm to handle uncertainty and hidden information effectively.

Action Space and Control

We define a comprehensive action space that allows the AI to perform all important in-game actions an individual player would. The action space is multi-dimensional, controlling the character’s movement, aiming, and use of weapons/abilities. For clarity, we break the action space into several sub-actions (or “heads”):
	•	Movement & Navigation: The AI can issue movement intentions such as moving forward/backward, strafing left/right, and adjusting its viewing angle (turning yaw/pitch). We may discretize movement into directions (e.g. 8 directional moves + a stop) and discretize aiming angles into a limited set of increments per time-step, or use continuous control for smoother aim. Jumping and crouching are additional binary actions. This results in a movement vector each tick (e.g. move direction plus whether jump/crouch) and a view rotation action. Constraints like movement speed and acceleration are enforced by the environment.
	•	Weapon Use: Actions to shoot or not shoot each frame. Rather than continuous firing, we treat “fire primary weapon” as a discrete action that can be taken if the crosshair is on target, etc. The AI must learn when to shoot. We also include weapon switching or reloading as needed. The agent might have an action to initiate a reload, or to swap to the sidearm if out of ammo. These are context-dependent but can be considered separate discrete actions (e.g. a “reload” action flag).
	•	Ability Usage: Each agent (character) has a set of abilities (flash, smoke, etc. as defined in the game). For each ability, the AI can decide to use it. Ability actions might be parameterized (e.g. where to throw a flash or place a smoke). We can break this down: first an action “use ability X” and if that ability requires targeting, additional continuous parameters for the target position or direction. To simplify, we might discretize common target locations (like pre-learned positions) or use a coarse grid. In our model, we will allow the network to output an ability command and a target relative location. Abilities that are instant or self-target just execute when chosen. The action space thus has components like “Use Ability 1 with target (dx,dy)” etc., but to start, we may limit to simpler representations (like always throwing utility in the direction currently faced).
	•	Communication (Optional): To simulate teamwork, the AI could have a discrete action to issue a comms message (like a radio call or ping). For instance, if it sees an enemy, one action could be “call out enemy spotted at [location].” The content of comms can be templated (spotting, requesting help, suggesting a strategy). This helps coordinate multi-agent play. During training, these comms would just update the teammates’ observation (going into their comms buffer). Including communication actions would increase complexity; if needed, we can first train without explicit comms and rely on emergent coordination, then add this later for more advanced teamwork.
	•	Buy Phase Actions: At the start of each round (during the buy phase), the agent has a different action set: purchasing weapons, shields, and abilities. We handle this by treating the buy phase as a separate step with its own decision output (e.g. choose a primary weapon to buy, yes/no on shield, which abilities to refill). These can be formulated as a series of discrete purchase decisions or a combined purchase strategy output. Given economy management is critical to Valorant, we will incorporate it in the training episodes (the agent learns when to save money vs buy out). The state will indicate it’s in buy phase and include available choices; the action output will correspondingly be interpreted as purchase decisions. Once the round starts, the action space switches to the movement/combat actions described above.

Action Execution Frequency: The AI will output an action at a fixed interval (e.g. every few game ticks). We may simulate a frame rate for decisions, such as 10-20 decisions per second, to mimic human reaction limits. This also helps in smoothing control. We might subdivide aiming (where continuous fine control is needed) versus higher-level decisions (like to use an ability) which might not need to be decided every single tick. In practice, the policy network can output all action components each tick, and we simply ignore or carry over ones that aren’t frequently changed.

Multi-Head Action Structure: For learning, it’s convenient to break the action output into multiple heads corresponding to independent decision facets (movement, looking, shooting, abilities, etc.)￼. We will design the neural network to have multiple output heads:
	•	A movement vector (or discrete move direction).
	•	A continuous or discrete look angle adjustment.
	•	A binary trigger for shooting (shoot vs. not shoot).
	•	A discrete selection for ability usage (including “no ability” as an option), plus targeting parameters if an ability is selected.
	•	Possibly a discrete slot for communication or other tactical decisions.

These outputs together fully determine the agent’s behavior each step. During execution, we enforce rules (e.g. you can’t shoot while in buy phase, etc.) and invalid combinations are either filtered out or never learned due to reward structure.

Why this granular approach? We need the AI to learn both low-level skills (aiming, movement) and high-level decisions (when to rotate to another site, when to use ultimate). A hierarchical action space might emerge implicitly: the network could learn to output meaningful combos (e.g. move towards cover and reload, or peek and shoot). If needed, we can also incorporate a hierarchical policy approach (first choose a high-level action like “attack/defend/rotate” then low-level execution). Initially, we keep it flat but structured as above.

In summary, the action space is rich enough to cover all player behaviors, and by decomposing it into sub-actions, we allow the learning algorithm to manage the complexity. The design aims to let the AI discover sophisticated tactics (like jiggle-peeking corners, coordinated rushes, or holding angles with utility) purely from maximizing rewards in the environment.

Model Architecture

To handle the complexity of this environment, our AI model will use a deep neural network with an actor-critic architecture (suitable for policy gradient methods). The model architecture will incorporate memory (recurrent layers) and possibly attention mechanisms to deal with partial observability and multiple entities. Key components of the architecture:
	•	Observation Encoding: We will first encode the raw input features (as described in the Observation Space) into a vector of fixed size that the network can process. Different parts of the observation may be encoded by specialized sub-networks:
	•	Spatial/Positional data: Coordinates (like player positions) can be encoded via a simple MLP or even a small convolution if we project positions onto a 2D map grid. Alternatively, we might use a vectorized representation of entities: e.g., encode each visible enemy’s relative position, health, etc., via a small feed-forward network into an embedding vector. Similarly for each teammate. We could then use a self-attention (transformer) module to allow the model to flexibly attend to an arbitrary number of teammates/enemies. A transformer-based encoder could take a set of entity embeddings (teammates, enemies, objectives) and produce a combined context embedding, naturally handling variable numbers of entities. This is an “if applicable” enhancement – we may start simpler and move to transformers if we need more scalability in representing many objects.
	•	Scalar data: Such as health, ammo counts, or timers, can be fed in either as normalized numeric inputs to the network or concatenated after some linear transformation.
	•	Vision/Audio events: We will encode discrete events like “heard footsteps at (x,y)” into a suitable representation (perhaps a two-value encoding: type of sound + approximate direction or distance). This could be treated similarly to other entities (as a short-lived entity event).
	•	The outputs of these encoders are concatenated or pooled to form a state representation vector. For example, we might concatenate: [self_state_embedding, aggregated_team_info, aggregated_enemy_info, objective_info, etc.]. If using attention, the final layer of the transformer could produce a pooled context embedding.
	•	Recurrent Memory Core: Since the agent needs to remember information over time, we include a recurrent layer such as an LSTM (Long Short-Term Memory) or GRU in the architecture. At each time-step, the encoded observation is fed into the LSTM along with the previous step’s hidden state, producing an updated hidden state. This lets the agent carry information forward – e.g., remembering that “the last known enemy position was behind that corner” even if currently out of sight. Recurrent policies are a standard approach for partially observable environments￼￼. If we use a transformer encoder for observations, we might still wrap an LSTM around the whole thing to maintain an internal state. Alternatively, a more cutting-edge approach is to use a Transformers-as-memory (e.g., a transformer decoder that attends over a window of past states), but that can be resource-intensive. Initially, an LSTM is a proven, simpler choice to give the network temporal memory.
	•	Policy and Value Heads: We adopt an actor-critic algorithm, so the network will output:
	•	Policy logits for each action sub-head (movement, firing, abilities, etc.).
	•	Value estimate for the current state (used for critic to calculate advantage).
The policy heads are typically produced by a final layer (or small layers) that take the LSTM’s output hidden state. For example, one linear layer might map to movement action probabilities, another to ability usage probabilities, etc. Similarly, a linear layer produces a scalar value estimate. The entire network’s weights are shared across all agents (each agent just has its own hidden state for the LSTM). A shared network with an agent-specific input is a known technique to allow one model to control multiple agents without them all behaving identically￼. We will include identifiers in the observation like a one-hot for which agent or role this is, or the team side, so that the network can learn different behaviors for different roles if needed, despite weight sharing.
	•	Parameterization for Personality: To achieve the personality-based behaviors, we will introduce an input parameter (or embedding) representing the desired play style. For example, we can have a continuous or categorical input that indicates “aggressiveness” or weighting of kills vs. objectives. This could be a simple two-dimensional input (e.g. [kill_weight, win_weight]) or a more abstract “personality embedding” that corresponds to a certain style. During training, we can feed different values here for different agents (or vary it across episodes) so the network learns to condition its policy on this input. The result is a single network that can produce a family of behaviors depending on the personality vector provided. This is analogous to conditioning a policy on a “goal” or context in multi-objective RL. Research has shown that adding such conditioning (like agent-specific or goal-specific inputs) allows one policy network to effectively represent multiple behaviors￼￼. In our case, the personality input will effectively act like an agent-specific context that skews decision preferences. For example, a high “kill_hungry” setting might lower the AI’s threshold to take fights versus falling back.
	•	Model Size and Complexity: We anticipate the network will be fairly large to handle the complexity – e.g., an LSTM with a hidden size on the order of hundreds of units, and a few fully connected layers or attention layers for encoding observations. If we incorporate a transformer, it might have a few self-attention layers (with moderate embedding dimensions to keep it efficient). The exact layer sizes will be tuned based on training stability and performance. We will also consider weight sharing vs specialized subnetworks for different phases: e.g., the buy phase decision could either be handled by the same network (with input indicating “in buy phase”) or a separate small network. To keep things unified, we lean toward a single network that can handle both the buy phase and combat phase, again distinguished by context inputs (phase indicator in observation). This ensures the AI can connect its buy decisions to its play style (e.g. a aggressive player might prefer buying weapons that suit high frag play).

Centralized Training (for Critic): During training, we might use a technique from multi-agent RL: a centralized critic. This means that while the policy (actor) sees only local observations, the value function (critic) could be given extra global information during training (like the full game state or positions of all players) to help it compute accurate credit assignment. This does not affect the executed policy (which remains decentralized and only uses local info), but can improve learning efficiency in cooperative settings. For our competitive environment, we may use one critic per team that gets global team observations. However, since the game is adversarial, we might instead rely on self-play and treat the environment as part of the agent’s own learning, without a centralized critic. This detail can be iterated on – initial approach: use the same network to produce value estimates from local observation (with perhaps some global hints like the last known enemy count), and if credit assignment appears problematic, explore a more centralized critic.

Why Actor-Critic? Actor-critic methods (like A3C, PPO, etc.) are known to be stable for continuous action environments and can handle stochastic policies well. They also allow using entropy regularization to encourage exploration. Given our complex action space, a policy gradient approach is appropriate since it can directly handle discrete and continuous mixed actions and optimize for long-term goals (win the round) with sparse rewards. We will likely use PPO (Proximal Policy Optimization) as our training algorithm, as it is a stable and scalable choice that has been used in large-scale game AI projects (e.g., OpenAI Five for Dota, Arena Breakout FPS AI￼). PPO’s clipped loss and ability to run in parallel environments make it a good fit.

In summary, our model architecture is a deep recurrent policy network, possibly enhanced with attention, that takes in a rich observation and outputs a multi-part action. It is designed to be one master model controlling all agents (with conditioning for individuality). This approach ensures that improvements to the model benefit all bots and that we can capture a wide range of behaviors in one training process rather than maintaining separate models for each behavior.

Training Regime and Self-Play Setup

Training an AI of this sophistication will require a carefully orchestrated regimen. We plan to use self-play reinforcement learning with a large number of parallel simulations to generate experience. The training regime includes:
	•	Self-Play Matches: We will pit teams of AI agents against each other in simulated matches. Typically, we will have 5 AI vs 5 AI (to learn both teamwork and opposition). To ensure the policy doesn’t overfit to a single opponent strategy, we will randomize initial conditions and periodically update opponents:
	•	At the start, all agents use the same initial policy (which could be random or some basic heuristic to at least move and shoot). They play many rounds against each other and improve via RL.
	•	As training progresses, we can maintain a population or league of policies￼ – e.g., keep snapshots of the agent at different iterations and randomly mix them in matches. This prevents two identical agents from co-evolving exploitable quirks and encourages robustness. OpenAI’s Dota2 training did this by playing against past versions (Fictitious Self-Play), and DeepMind’s Quake III work trained a population concurrently￼. We can implement a similar idea at a smaller scale: periodically, save a “checkpoint policy” and use it occasionally as the opponent or teammate, so the current learning agent has to perform well against a variety of behaviors, not just a mirror of itself.
	•	Episode Structure: We have to decide whether one training episode = one round or one full match (multiple rounds). A full match (first to 13 rounds) is a very long horizon with sparse reward (only after match result), which is difficult for RL. Instead, we will treat each round as an episode for the learning algorithm. Each round has a clear end (win/loss or timeout) and a reward can be given at that point. However, to capture economy and multi-round strategy, we will carry state between rounds: the agents’ observation at round start will include their money and last round outcome, and their policy should learn that saving or spending affects future success. We can achieve this by slightly extending the episode to cover multiple rounds or by giving intermediate rewards for economy preservation. A compromise: use rounds as episodes but include a small reward for economy (e.g., leftover credits) to influence buy behavior. Alternatively, train in phases: first focus on round-level combat tactics; later, incorporate multi-round strategy via either curriculum or a higher-level decision module.
	•	Curriculum Learning: The environment is extremely complex for a cold-start (agents have to learn aiming, teamwork, map navigation, etc. all at once). We will employ a curriculum to gradually increase difficulty:
	1.	Basic Combat Training: Start with simplified scenarios, e.g., 1v1 or 2v2 on a small map. This allows the agents to learn fundamental aiming and movement in duels. The reward here can heavily weight getting a kill and winning the mini-round. No abilities or limited abilities might be used initially.
	2.	Team Tactics: Scale up to 5v5 on a full map but perhaps without the spike (objective) initially. Agents learn to coordinate in team deathmatch style rounds – focusing on trading kills, flanking, etc. We can introduce basic rewards for staying alive and team kills.
	3.	Objective Play: Add the spike planting/defusing objective once combat is learned. Now agents must learn to attack/defend bomb sites and handle the additional win conditions. At first, maybe give dense rewards for performing the objective (planting or defusing successfully) to guide behavior.
	4.	Abilities and Full Game: Finally, enable the full ability toolkit and economy. By this stage, the agents have a foundation in shooting and teamwork, so they can explore using abilities to gain advantage. The reward shaping will be adjusted to the final design (see Reward section) to account for all aspects (kills, round wins, etc.).
This curriculum can be implemented by training sequentially, or by starting all aspects but with shaping that emphasizes simpler skills first. We can gradually reduce shaping rewards as competency improves, letting the agent focus on the sparse main objective (round wins).
	•	Reward Signal and Optimization: We will use the reward function detailed in the next section, and optimize the policy to maximize expected cumulative reward. The training uses PPO (with mini-batch updates and epochs per batch). We will tune hyperparameters like learning rate, entropy bonus (to maintain exploration), and gradient clipping. We also use discount factor (γ) appropriate for the episode length (likely high, e.g. 0.99, since a round can last up to 100 seconds of simulated time).
	•	Parallel and Distributed Training: To get enough experience, we plan to run many game instances in parallel (potentially dozens or hundreds, depending on compute). Each instance runs a match with agents using the current policy (or a mixture of current and past policies as noted). The experiences (state, actions, rewards) are collected, and the neural network is updated asynchronously or in batches. Modern RL frameworks (like SEED RL, RLlib) allow distributed training, where rollout workers generate data and a central learner updates the model. We will leverage such a setup to scale training. Population Based Training (PBT): We might also employ techniques like PBT￼ where multiple training runs with slightly different hyperparameters or reward weightings learn in parallel and occasionally swap information, to find a better optimum. This is an advanced optimization to consider if standard training plateaus.
	•	Evaluation During Training: Periodically, we will evaluate the current model against some fixed benchmarks (e.g., a scripted bot or an earlier snapshot) to track improvement. If the AI starts significantly beating the past versions and achieving high win rates, we know it’s learning effectively. We will also watch for signs of mode-collapse (agents converging to a deterministic pattern) or degenerate strategies (like overly camping or repeated suicide rushes if reward shaping is off) and adjust accordingly.

Throughout training, exploration will be maintained by the stochastic nature of the policy and explicit entropy bonuses. Early on, agents will behave almost randomly but over time, through millions of rounds, we expect them to discover tactics that improve their win odds, which the learning algorithm will reinforce. Given the complexity, training could require billions of frames of experience to reach top-tier play. We will use interim milestones (e.g., “AI can beat a medium-difficulty scripted bot” or “AI can execute a coordinated site push 70% of the time”) to gauge progress.

By the end of this training regime, we aim to have a set of neural network weights that encode a high-level of game sense and mechanical skill for each agent. The next sections detail critical aspects of the training: the reward design that drives the learning, and how we ensure variety in behavior through personality conditioning.

Reward Design and Personality Conditioning

Designing the reward function is pivotal – it drives the AI’s behavior. Our reward scheme will combine the primary objective of winning rounds with secondary objectives that encourage good play and allow personality-based tuning. The reward function will be parameterized so we can adjust the weights for different “personality” profiles. The general form:

 R = w_{\text{round}} \cdot R_{\text{roundOutcome}} + w_{\text{kills}} \cdot R_{\text{kills}} + w_{\text{teamplay}} \cdot R_{\text{teamplay}} + \ldots 

Where the w’s are weights that we can vary per personality. Below are the components:
	•	Round Outcome Reward: A large positive reward for winning the round (for each team member), and a negative or zero reward for losing. This is the primary sparse reward that ensures agents care about winning above all. For example, +100 reward for each player on the winning team at round end, and 0 for the losers (or -100 to emphasize avoid losing). We may normalize these values, but conceptually it should dominate in the long run. The existence of this reward ensures the policy’s ultimate goal is to maximize round wins (which translates to match wins).
	•	Kill Reward: A moderate reward for each enemy kill the agent achieves. This encourages combat effectiveness. We might give, say, +10 for a kill. We can also give partial credit for assists (e.g., +5 for an assist) to promote teamwork in focus-firing enemies. However, if left unchecked, a bot might chase kills at the expense of objective play – that’s why the weight w_{\text{kills}} will be tuned per personality. A “kill-hungry” personality will have a relatively higher kill reward weight, meaning that version of the AI will take more risks to get frags. Conversely, a “team-first” or support personality might have a lower kill reward weight and higher weight on round win or assists.
	•	Death Penalty: To avoid reckless behavior, we can assign a small negative reward for dying (e.g., -5 or -10). This is balanced carefully; too high and agents become overly timid. But a modest penalty makes agents value their life. We might vary this with personality: an aggressive agent might almost ignore death penalty, while a survivalist agent has a higher penalty, making it avoid risky plays.
	•	Objective and Teamplay Rewards: We include rewards that promote playing the objective and helping the team:
	•	Spike Plant/Defuse: If the agent plants the spike successfully, give a reward (e.g., +10). Same for defusing. Possibly reward partial credit for starting a plant/defuse to encourage attempting it (though the round win is the big reward if it succeeds). For a support-focused personality, we might boost these rewards further, as they indicate playing for the team’s victory.
	•	Round Contribution: We could give a small reward to all team members for a round win beyond the outcome reward (like splitting some bonus among survivors or participants). However, since all winners get the same round outcome reward, explicit team reward may be redundant. One twist: we might reward team survival, e.g., +2 for each teammate alive at round end, to encourage coordination and saving allies (this could be part of a “teamplay” reward component).
	•	Damage/Assist: Instead of only kills, we can reward damage dealt to enemies (scaled appropriately) to credit situations like doing 90 damage but a teammate finishes the kill. Similarly, credit for assists as mentioned. This ensures the AI doesn’t mind if a teammate secures the kill – it still gets some reward for contributing.
	•	Utility Use: We might add reward shaping for proper use of abilities. For example, successfully blinding an enemy with a flash or gathering intel with a recon ability could yield a small reward. This is tricky to get right but could be based on events (like if an ability leads to an enemy being revealed or killed, reward the ability user). In early training, we can include such rewards to teach ability usage (e.g., +3 for a flash that affects an enemy, +5 for using smoke to block a chokepoint). Later, these can be faded out so that the agent doesn’t abuse abilities for reward if not needed, but initially it helps them learn the value.
	•	Penalty for Team Damage or Poor Play: If friendly fire exists, we penalize hurting teammates. We might also penalize things like extreme camping if it leads to timeouts (though better is to structure it so that not winning the round is its own punishment). For example, if a round ends due to time with objective not planted (attackers failed), we could give attackers a -20 penalty distributed, to teach them that doing nothing is worst. Similarly, defenders letting time run out is fine, but if spike explodes, defenders get the lose penalty anyway. These situational penalties are part of making sure the AI learns the right incentives.
	•	Living through the round: In some cases, we might reward staying alive until round end for the winning team (since saving guns is important in Valorant economy). But that might complicate things. It’s possibly not needed because winning already implies they likely survived or traded effectively.

All these rewards happen at certain events (kill event, death event, round end, etc.). We will normalize and balance them so that across thousands of rounds, the policy’s optimal behavior is to win rounds efficiently – but by tuning weights, we can get varied styles:
	•	A “Rambo” (kill-hungry) agent: Increase kill reward and decrease death penalty. It will aggressively seek out frags, possibly at the cost of some round losses if it overextends. It might also not prioritize planting if it can chase kills. This could model a star duelist who cares about getting high kill counts.
	•	A **“Supportive” or “Strategist” agent: Put high weight on round win, objective actions, team survival, and maybe personal kills are given less weight. This agent will play more cautiously, stick with teammates, and focus on utility usage and staying alive to win the round. It might even value an assist nearly as much as a kill.
	•	A “Balanced” default agent: weights tuned to something in between, perhaps roughly: win round = 100, kill = 10, death = -5, plant = 10, assist = 5. This agent will generally play to win but also take fights when advantageous.

We will implement personality conditioning in training by randomly assigning a personality profile to each training agent in each match. For example, in some self-play matches, both teams might be all balanced; in others, one team’s agents have kill-hungry parameters and the other team more team-first, etc. The personality vector (weights) will be part of the agent’s observation (so the agent knows its own preference setting) and its reward is calculated according to that setting. Over time, the network learns to respond to the personality input – effectively learning a family of policies￼￼. This technique is akin to multi-objective or conditional RL, where an agent can adapt to a parameterized reward goal. It has been shown that by rewarding trait-aligned behaviors, one can induce diverse behaviors in agents￼￼. Our approach mirrors that: we explicitly shape rewards to emulate “personality traits” and gather a diverse set of behaviors.

One challenge is to ensure that extreme personalities don’t destabilize training (e.g., an overly aggressive reward might cause an agent to just chase kills and die, providing poor learning signals). We will keep all personalities oriented towards ultimately winning (round win always carries weight) to some degree. And we may limit the range of weights (e.g., kill weight could vary from 5 to 15 around a base, rather than 0 to 100).

During inference (when the AI is actually controlling bots in a live game), we can select a personality setting for each bot to achieve desired behavior. For example, we might set an easy-mode bot to a timid, team-first style (making it a bit passive), or have a bot known as a “fragger” with kill-hungry style to spice things up. We simply feed the corresponding personality vector to the model for that agent, without needing to retrain.

In summary, the reward design is multi-faceted and tunable. It ensures the AI learns the core objective (win rounds) while also being able to exhibit different styles by emphasizing certain rewards. By training on a mix of these, our model will be versatile. This reward shaping is crucial to achieving “polished” performance – it’s not just about winning, but how the agent wins (with smart tactics, using abilities, teamwork, etc. which we encourage through rewards). We will refine these rewards as training progresses (if we see unintended behaviors, we adjust weights accordingly, a process sometimes called reward shaping iteration). The result will be agents that not only win, but do so in a way that looks intentional and characteristic, enhancing the believability of our AI players￼￼.

Difficulty Scaling and Skill Levels

We want the AI to be enjoyable for players of different skill levels – from novices who need practice up to experts who want a serious challenge. Our design will allow scaling the difficulty and skill of the AI through several mechanisms:
	•	Adjusting Reaction Speed and Precision: One straightforward way to vary difficulty is to simulate human reaction times and accuracy. Our top-tier AI can potentially react instantaneously and aim with superhuman precision (since it’s neural and not subject to human limitations). To create easier levels, we will introduce response delays or inaccuracy. For example, for a “Beginner” AI setting, we can add a slight delay (e.g., 200 ms) between when the AI decides to shoot and the shot is fired, to mimic slower reflexes. We can also add random aim error – e.g., slightly perturb the target aim point so it misses occasionally. These tweaks don’t require changing the core policy; they are post-processing on the actions to tone down performance. Even DeepMind’s human-level Quake bots constrained their reaction times to human-like speeds￼, and we’ll do similarly. This ensures that “Easy” bots don’t insta-flick headshot players, which would be frustrating.
	•	Using Different Model Checkpoints: During training, the AI will gradually improve. Earlier in training, it will exhibit more mistakes and simpler tactics – which might actually correspond to a lower skill bot. We can save models from intermediate training phases and label them as lower difficulty AIs. For instance, after training for some hours, the AI might reach the level of an average player; that model could be used as a “Medium” bot. The final model after full training is “Hard/Expert”. This approach was used in past projects where agents at various training iterations provided a natural skill ladder. We have to ensure these intermediate models are stable enough to deploy; we might further fine-tune them slightly to remove any obvious training-time quirks. But overall, leveraging the training progression gives us a spectrum of skill without extra work.
	•	Altering the Reward/Personality for Less Skillful Play: Another approach is to deliberately train or configure some bots to play “worse” in strategic ways. For example, an easy-level bot might have a personality that heavily penalizes dying, making it extremely passive – easy for players to outmaneuver. Or we could reduce how much it values kills, so it might not capitalize on opportunities aggressively. However, this could make it too odd. Instead, we can simply not fully train some models or train a variant with limited capacity:
	•	Reduced Training Time or Capacity: We could stop training early (as above) or even train a small network version for easier bots. A smaller network will inherently not be as capable at the complex tactics, capping its skill. This could be our “Easy AI network” – less computation, less cunning.
	•	Different Exploration/exploitation settings: A beginner bot might keep a higher entropy or random action probability even when deployed, to simulate confusion or mistakes. For example, we could make it occasionally choose a non-optimal action (epsilon-greedy style). This can be done by sampling from the policy with more randomness or by blending the learned policy with a random policy occasionally. On higher difficulties, we sample greedily (taking the top action each time, resulting in more optimized behavior).
	•	Parameterizing Difficulty: Similar to personality, we can have a difficulty parameter that influences behavior. This could tie into the personality vector or be separate. For instance, an “aggression” personality is orthogonal to “skill” in execution. We might give the model a notional “skill level” input during training – but that’s tricky, as we mostly want to degrade skill, not have the model learn something new. It might be easier to manage difficulty outside the neural network (via the methods above). One thing we can do is ensure the AI’s observation or action is handicapped: e.g., an easy bot might have a narrower field of view or a shorter hearing range configured in the simulation, to emulate a less aware player. But that’s an artificial way; adjusting reaction and aim is more straightforward and effective.
	•	Game Balancing Tools: We will also expose some tuning knobs that are easy to adjust in practice for balance:
	•	Aim accuracy scalar: how perfectly the bot shoots at the target (we can make it intentionally miss some percentage).
	•	Decision delay: how often the bot updates its decisions. Easy bots might update movement and aiming less frequently, making them more sluggish.
	•	Economic behavior: easy bots might make suboptimal buy choices (wasting money or not buying best weapons), while hard bots always make optimal buys. We can enforce this by sometimes overriding their buy decisions for lower levels.
	•	Ability usage: reduce the frequency or effectiveness of ability usage for easier bots (so they feel less overwhelming). For instance, an easy bot might rarely use its flash or use it in a predictable manner.

Our recommended approach to scaling difficulty is to train one strong model, then derive weaker versions by either using earlier snapshots or applying handicaps. This way, we ensure even the easy bots behave using the same fundamental logic (just with more errors), rather than writing entirely different AI. Consistency in behavior is good – a player should feel the bots all follow the same “rules of engagement,” just that some are more skilled than others.

During testing, we’ll calibrate these difficulty settings by playing against the bots and perhaps using metrics (like win rate of an average player against them). We might adjust, for example, the reaction time delay until a typical player can reliably beat an “Easy” bot but struggles against a “Hard” bot.

Additionally, because our AI is parameterized by personality, we can combine difficulty and personality for interesting combinations: e.g., Easy-aggressive (a bot that rushes but has poor aim) vs Hard-passive (a bot that rarely makes mistakes but plays defensively). This gives game designers a lot of flexibility to create engaging PvE scenarios or bot personalities for story modes.

In conclusion, difficulty scaling will be achieved by controlling the AI’s effective speed, precision, and decision optimality. We will offer multiple levels by leveraging the training timeline and simple post-processing on the AI outputs. This ensures that players of all skill levels can have a fun and fair experience, and it showcases the AI’s range – from making human-like errors to executing near-perfect tactics at the highest level.

Exploration and Ongoing Improvement

Training such a complex AI requires ensuring sufficient exploration of strategies and continuous policy improvement without getting stuck in local optima. Here’s how we handle exploration and refinement during training:
	•	Intrinsic Exploration Incentives: We will include an entropy bonus in the reinforcement learning objective. This means the agent is encouraged to keep its action distribution somewhat unpredictable, especially early in training. A high entropy bonus at the start leads to wide exploration of the action space (bots trying varied tactics). As training progresses, we might decay this bonus so the policy can converge to more deterministic, effective strategies, but we never remove it entirely to avoid premature convergence.
	•	Randomized Training Conditions: We will introduce randomness in the training matches to encourage the AI to handle a variety of situations. For example: randomize spawn locations (within usual spawn areas), occasionally change team compositions or agent roles, randomize which bomb site the spike is at initially, etc. We could even vary the map layouts slightly (if our game will have multiple maps, training on all of them in random rotation prevents overfitting to one). This domain randomization forces the agents to explore different tactics rather than exploiting one fixed scenario.
	•	Curriculum and Staged Goals: As described, the curriculum moves the goal posts gradually. This inherently provides exploration at each stage (since when a new element is introduced, the agent has to explore how to deal with it). For instance, when we first enable abilities, the agent might randomly use them in odd ways until it discovers effective uses that yield reward (like learning that using a smoke to cover an approach leads to more round wins). Each new element is an exploration opportunity.
	•	Multi-Agent Strategy Exploration: In self-play, agents co-adapt. To avoid them converging to a dull equilibrium (e.g., both teams always camp and tie), we use the population/league approach. We will periodically freeze a copy of the current policy (call it A) and continue training a new instance (call it B) from it. Then pit A vs B. If B finds a way to beat A (improved strategy), we keep B and perhaps discard A or keep it as a challenger. This is analogous to how AlphaStar maintained a league of competitors to keep training diverse. In simpler terms, we ensure there’s always a mix of opponents employing different strategies, pushing the main policy to adapt and improve continually.
	•	Avoiding Exploitation of Bugs or Reward Loops: We have to monitor for any unintended reward exploits. For example, an agent might learn to abuse the reward system (maybe by flashing a teammate repeatedly if we incorrectly rewarded any flash usage). Through careful reward shaping we minimize this, but if something slips through, we’ll revise the reward or environment rules. Our testing during training (and having some scripted opponents occasionally) will help identify bizarre behaviors. If an agent does something odd that technically maximizes reward but is undesirable in gameplay, we’ll address it (classic example: agents learning to just hide to not die if the reward heavily penalizes death – we counter that by ensuring winning is far more important).
	•	Algorithmic Stability: We chose PPO which is stable, but we also consider using KL divergence limits or adaptive learning rates to ensure the policy doesn’t change too radically in one update (which can kill exploration by converging too fast or oscillating). We’ll keep an eye on training curves for signs of divergence or collapse (e.g., sudden drop in entropy or reward spikes).
	•	Exploration vs Exploitation Trade-off: Initially, exploration is key; later, we want exploitation (refining the best strategies). We can slowly shift the balance by lowering entropy bonus and perhaps using playout at test time where the agent plays more greedily. One advanced idea: use curiosity-driven exploration – an intrinsic reward for exploring new states. This might help in a large map where encountering enemies can be sparse. We can use techniques like Random Network Distillation or state visitation counts to reward agents for checking corners or trying new routes. This could prevent, say, an agent from always rushing the same route every round because it never tried the other route. Such intrinsic rewards would be a behind-the-scenes aid to ensure the agent has knowledge of the whole map and various strategies. We’ll consider this if we see exploration stagnating.
	•	Continuous Learning and Updates: Even after we deploy a trained AI in the game, we want to update it periodically. We will keep the training environment and periodically incorporate new data. For example, if the game introduces a new map or a new agent character with different abilities, we can update the simulation and continue training the AI to handle those. The training pipeline thus remains active (though maybe at lower intensity) to improve the AI over time. This can also allow the AI to adjust if it encounters novel strategies (even from human players). We could use game telemetry – e.g., if we find players consistently exploit a weakness in the AI, we can incorporate those scenarios in training (either through self-play – the AI playing as the humans did – or even imitation learning from those human actions to show the AI a better response).
	•	Evaluation and Iteration: Throughout the project, we’ll evaluate the AI’s performance not just by training reward, but by actual gameplay metrics (win rate against certain benchmarks, how “intelligent” the plays look via replays, etc.). We may identify areas to improve (like “AI doesn’t understand one-way smokes well” or “AI never uses a certain ability effectively”). Then we adjust either the inputs (give it better info about that ability), the rewards (explicitly reward using that ability correctly a few times), or even add a specialized network output if needed (maybe a special head to decide when to use that specific tactic). This iterative refinement ensures the final AI is polished in all aspects.

In conclusion, we have a robust plan to ensure exploration (so the AI can discover creative strategies) and continuous improvement (so it keeps getting better and doesn’t plateau too early). Self-play with a mix of opponents and carefully tuned RL algorithms will drive the policy towards an optimal, while our interventions (entropy, intrinsic rewards, curriculum) will keep it from getting stuck. The end result should be an AI that has explored the space of tactics broadly and settled on strong, effective behaviors that appear highly skillful and adaptable.

Model Persistence and Lifecycle

Building the AI model is not a one-and-done task; we need to manage its lifecycle: saving, updating, and deploying versions reliably.
	•	Checkpointing and Version Control: During training, we will frequently checkpoint the model (saving weights and optimizer state). This allows us to roll back if a training run goes awry and also to have multiple versions of the model for different purposes (as mentioned, we might use earlier checkpoints for easier difficulties). We will tag certain checkpoints as candidates for release once they pass evaluation criteria. Each candidate will be versioned (v1, v2, etc.). We maintain a change log of what changed (e.g., “v1.2 – improved corner checking behavior, fixed aggression balance”). This is important for debugging and for the design team to test specific AI builds.
	•	Deployment Considerations: The trained model (or models) will be integrated into the game. We need to ensure the model runs efficiently in real-time. The architecture design considered this (keeping model size reasonable). We might further optimize by converting it to a inference-efficient format (e.g., ONNX or TensorRT for optimized runtime). We will test the model’s inference speed with typical game hardware. If the LSTM and attention are too slow per frame, we might do some optimizations like quantization or reduce the frequency of decisions slightly. Ideally, though, we targeted a manageable network size so that even 10 agents (5v5) running simultaneously is fine on a server or client machine.
	•	Dynamic Difficulty or Updates: We can ship multiple difficulty versions of the model as separate files, or a single model with parameters. At runtime, depending on chosen difficulty, we load the appropriate model or adjust the parameters for the unified model. This flexible design (one model, many personalities/difficulties) means we might just ship one network and provide it different input settings.
	•	Continual Training and Patches: Once the game is live, we expect to do further training to improve the AI. This could be in response to:
	•	Discovered Weaknesses: Perhaps players find that bots are easily lured into traps or have poor endgame clutch performance. We can address that by adding scenarios in training and continuing the training from the latest model (fine-tuning).
	•	New Content: New maps, weapons, or agent characters will require the AI to adapt. We’ll extend the simulation to cover new content and either retrain from scratch or (more efficiently) fine-tune the existing model on the new content. Fine-tuning with a smaller learning rate on new scenarios will adjust the policy without forgetting previous knowledge (we may use strategies like replaying old scenario data to avoid catastrophic forgetting).
	•	Meta Changes: If game mechanics are tweaked (say weapon damage or economy changes), the AI should ideally adapt. We can simulate the new balance and run additional training to let it re-learn optimal strategies under the new conditions.
	•	A/B Testing AI Improvements: For major changes, we can A/B test new AI versions with a subset of users or in offline evaluations. This is to ensure the new version is indeed better (e.g., does it provide a more fun experience? Is it noticeably smarter?). We might measure things like average round length, damage dealt by bots, etc. to see differences. If positive, we roll it out to all.
	•	Stability and Regression: Each new model version should be tested to make sure we didn’t introduce regressions (e.g., the new version shouldn’t suddenly be worse at planting the spike due to some change). Our evaluation suite will include a battery of test matches: bot vs bot, bot vs some scripted behaviors, and possibly bot vs a small group of internal testers. We’ll compare key stats to previous version. If something regressed, we investigate and perhaps retrain with adjustments.
	•	Persistence of Learning (Online?): We currently plan for offline training (not learning during live matches with players, to avoid unpredictability). However, a future idea could be to allow bots to very slowly adapt online (for example, track player behavior and update some strategy selection). But this raises consistency issues and is generally not done in shipped games due to fairness. So we will likely stick to offline improvements and periodically publish updates.
	•	Data Storage and Tooling: We will store not just the models but also the training data samples (or at least aggregated stats) to help in future training. If we have human replays, we can use them to evaluate the AI (e.g., how would our bot fare in a certain recorded round). We’ll also implement tools to visualize the AI’s behavior – such as heatmaps of where it tends to go on a map, or what inputs it had when it made a certain decision – this helps developers trust and verify the AI.
	•	Safety Checks: Since the AI learns behaviors, we should ensure it doesn’t find any glitch in the game to exploit. We will simulate as close to the real game as possible, but if the AI tries something unexpected (like exploiting clipping or perfect grenade lineups that weren’t intended), we might have to explicitly disallow those in the environment or handle them. Persistence means always being vigilant for such issues and retraining or constraining if needed.

Overall, our approach treats the AI model as a living component of the game: we will iterate on it, maintain it, and update it just like any other feature. The infrastructure to train and deploy updates is a core part of the plan. This way, the AI can evolve even after launch, keeping the experience fresh and challenging. It also means if players start beating the AI consistently, we have the ability to raise the bar by giving the AI more training – possibly even surpassing its initial capabilities, as often demonstrated in research where longer training yields better results.

Evaluation Metrics and Testing

Throughout development and training, we will use various evaluation metrics to measure the AI’s performance, learning progress, and behavior characteristics. These metrics guide us in tuning the system and ensure we meet the design goals. Key metrics include:
	•	Win Rate: The primary metric during self-play training is the win rate of one policy against another. For a single agent, we can measure the win rate of its team in training matches. As training progresses, we expect win rates to approach 50% in symmetric self-play (since both sides are equal) – so instead, we’ll use win rate against older versions or scripted bots. For example, we might have a baseline bot (with some simple strategy) and test our AI periodically against it; a rising win rate indicates improvement. Post-training, win rate of bots against human testers of various skill is the ultimate measure of difficulty appropriateness.
	•	Reward per Episode: Since the training optimization uses a reward signal, we track average reward per round (episode). This should trend upward as the AI learns. We break it down by components too – e.g., average kills per round (which correlates with kill reward), average round outcome reward (which is basically round win rate). Monitoring these helps ensure the agent is getting balanced exposure. If we see, say, the average kill reward dominating and round win not increasing, that could indicate the agent is fragging out but not converting to wins (maybe too selfish) – then we’d adjust rewards.
	•	Behavioral Statistics: We will log in-game statistics of the AI:
	•	Average kills, deaths, assists (K/D/A).
	•	Damage dealt and taken.
	•	Ability usage count (how often and which abilities are used).
	•	Objective interactions: plants, defuses attempted/successful.
	•	Movement patterns: distance traveled, time spent stationary vs moving.
These can be compared to human player benchmarks (if available from match data) to gauge if the AI is behaving within reasonable human-like bounds or doing something superhuman/unrealistic. For example, if the AI has an 80% headshot rate, that might be too high; we may dial down aim to more human levels for believability at certain difficulty settings.
	•	Teamplay Metrics: Since we want coordinated team behavior, measure things like: Trade kill percentage (does a bot often trade a kill when its teammate dies?), grouping (how often bots are near each other vs lone wolf), and synergy in executes (did multiple bots use utility in a coordinated manner?). Some of these are qualitative, but we can set up proxies, e.g., count rounds where bots enter a site within 2 seconds of each other as a coordinated push.
	•	Personality Differentiation: We’ll test that our personality parameter truly yields different behaviors. For instance, run the same scenario with a kill-hungry setting vs a team-first setting and measure differences: the kill-hungry bot should have higher average kills but maybe lower survival or round win rate if it’s too reckless; the team-first should maybe have more assists and higher survival. We can also survey observers (QA testers) if the behavior “feels” different in line with the personality description. If the differences are too subtle, we might amplify the reward weight differences; if they are too extreme and sabotaging performance, we tone it down.
	•	Difficulty Calibration: For each difficulty level we intend to ship, we will test the bots against players of corresponding skill. Metrics like average damage the bot deals to a player in a duel, reaction time measured (we can instrument how quick the bot fires after seeing an enemy), etc., will ensure the easy bot indeed reacts slower than a hard bot. We may also use an ELO or ranking system: pit different versions (easy, medium, hard) in round-robin matches to verify the hard bots consistently beat medium, which beat easy, etc., establishing a ranking. This sanity-checks our difficulty scaling approach.
	•	Stability and Robustness: We will test the AI in edge cases: e.g., 1vN clutch situations, unusual map positions, or when allies are all dead. Does it still behave reasonably? A metric could be “clutch success rate” (how often it wins a 1v2 or 1v3 situation). Or “save rate” (does it know to save a weapon when a round is unwinnable – advanced behavior we might train). We want to ensure the AI doesn’t break down or freeze in odd situations. In testing, if we find cases where the AI seems confused (e.g., running in circles), we will add those scenarios to training. Tracking any such failure incidents count is useful.
	•	Performance Metrics: Separately, ensure the model inference time and resource usage are within limits. Monitor CPU/GPU usage when running 10 bots, memory footprint of the model. While not a gameplay metric, this is crucial for deployment.
	•	Human Feedback: If possible, have players (or testers) play against the AI and provide qualitative feedback: Did the bot feel fair? Too predictable or sufficiently varied? Any obvious flaws noticed? This feedback is invaluable and often points to issues metrics alone might miss (like the bot might statistically be fine but maybe it always does a certain annoying exploit). We incorporate this into our evaluation loop.

By using these metrics, we will iterate on the training and design. For example, if the ability usage count is low, we know the AI isn’t using its utility enough – perhaps we increase the reward for using abilities or add more training scenarios focusing on utility. If the K/D ratio is great but win rate is not as high, maybe the bot isn’t playing objective – adjust rewards or strategy.

Ultimately, success is when the AI consistently demonstrates:
	•	Competitive performance (can beat strong human players at high difficulty).
	•	Varied and interesting behavior (not all bots playing identically every round).
	•	Tactical soundness (makes good decisions, uses abilities, covers angles, etc.).
	•	Adaptability (if one strategy isn’t working in a match, do they try another? We can simulate this by playing multiple rounds and seeing if the bot mixes up its approach or gets stuck).

All these will be documented in final evaluation reports as part of the technical design deliverable, showing that the AI meets the game’s requirements.

Alternative Approaches Considered

During the design of this AI system, we considered a few alternative approaches to structuring the models and training. Here we briefly outline them and explain why we chose our current path:
	•	Multiple Specialized Models (Per-Player or Per-Role Models): One idea was to train a different neural network for each agent role or each personality type. For example, have a “Duelist AI” vs a “Controller AI” since their play styles and abilities differ, or a separate model tuned specifically to be kill-hungry and another to be team-first. The advantage of specialization is that each model can be optimized for that particular behavior or role without having to accommodate others. In cooperative multi-agent contexts, another variant is training one model per team (a centralized team brain). We decided not to go with fully separate models for each agent for several reasons:
	•	Maintenance and Consistency: Maintaining many models would be burdensome. Any improvement made to one (e.g., better aiming) would have to be replicated across all. With one master model, improvements benefit all agent behaviors.
	•	Overlap of Skills: Many underlying skills (aiming, movement, basic tactics) are common to all roles. It’s more sample-efficient to learn them once in a shared model. Separate models could waste capacity relearning the same fundamentals.
	•	Coordination: If each agent had its own brain, getting them to coordinate might be harder unless we implemented explicit communication. With a shared model (or at least shared training), coordination emerges more naturally as they are essentially copies of the same brain (with maybe different inputs). Parameter sharing in multi-agent RL is known to be effective when agents share similar dynamics￼.
We do acknowledge that roles like a Sentinel (defensive) versus Duelist (entry fragger) have different priorities, but we handle that via input parameters (conditioning and rewards) rather than entirely different networks. This gives us the benefits of specialization while still training a unified model.
	•	Centralized “Mastermind” vs Individual Agents: Another approach in multi-agent systems is a single network that outputs actions for all agents at once (essentially treating the whole team as one big combined agent). We considered if a single model could take the state of the entire team and output all 5 players’ actions. While this could, in theory, enforce optimal team coordination, it has downsides:
	•	The action space and state space blow up (the network would need to output 5x the actions and take in 5x observations). This is very hard to scale and train.
	•	It’s not modular – if we wanted to drop-in a human player in place of one AI, a central model doesn’t handle that well.
	•	It reduces generality: we want each AI to function as an individual that could play alongside humans or other AI interchangeably.
So we kept the paradigm of decentralized agents (each with its own policy execution). They are united by common training and can communicate via simulated comms, but there’s no single “puppet master” controlling all. This aligns with how actual players operate and makes the AI more flexible in different game modes.
	•	Rule-based or Hybrid AI vs Pure RL: While our focus is on RL, we did consider a hybrid approach: use scripted rules for certain parts (like a high-level strategy module that decides which site to attack, and RL only for low-level combat). This could make training easier by injecting domain knowledge. However, it also limits the AI’s ability to learn surprising strategies and can require a lot of manual tuning of rules. Since a key differentiator for us is deep, learned behavior, we leaned toward a pure learning approach. We might still initialize some behaviors or constrain obvious things (like “don’t teamkill” as a hard rule), but overall, we trust RL and self-play to discover effective tactics. The success of end-to-end RL in complex games (Dota, StarCraft, FPS Capture the Flag) gives us confidence this is viable￼.
An alternative in the literature is Imitation Learning from human replays to kickstart the AI (as was done in AlphaStar and OpenAI Five). We opted to prioritize self-play RL, but we haven’t ruled out using any available human data to pre-train or shape rewards (for instance, if we have heatmaps of where humans tend to go, we could reward the AI for also checking those common angles). If training proves slow, imitation could be a fallback to speed up initial learning.
	•	Different Reward Schemes: We debated how granular or dense to make the rewards. One alternative was to use a purely sparse reward (only win/lose) and rely on the AI to figure everything out (as was done in some research – it can work but takes enormous training time). We decided to include shaped rewards (kills, etc.) to accelerate learning of good behaviors. Another idea was a more explicit multi-objective optimization where we train separate value functions for different objectives and then combine them. Instead, we settled on a single scalar reward that is parameterized by weights. This is simpler and allows us to adjust via those weights without changing the algorithm. As noted in research, rewarding trait-aligned behaviors explicitly is effective to achieve those traits￼, so we embedded that into the reward design.

Recommendation – Chosen Path: After weighing these options, we recommend the single, unified model approach with parameterized behavior, trained via self-play reinforcement learning with a carefully shaped reward. This approach maximizes reuse of learned skills, ensures consistency, and provides flexibility to derive many behaviors from one training run. It leverages state-of-the-art techniques from successful multi-agent RL research while being practical to implement and extend. Alternative methods like multiple models or rule-based systems were either less scalable or against the spirit of emergent, learned gameplay we want to achieve.

We will, however, keep some of those alternatives in mind as backups. For instance, if we find one network can’t cleanly handle two very different roles, we might split the policy slightly (say, have a minor network extension for that role). But the plan is to push the unified model as far as possible – benefiting from the full breadth of training data (all roles, all styles) to create a truly robust AI.

Conclusion

In this technical design document, we outlined the blueprint for a next-generation AI bot in a Valorant-style esports game. The design emphasizes using deep reinforcement learning and self-play to cultivate advanced behaviors under realistic game conditions. The highlights of the design are:
	•	A high-level architecture that uses a recurrent actor-critic neural network to process partial observations and output a full range of player actions.
	•	An observation space carefully constructed to only provide information a human player would have (vision, sound, communications), forcing the AI to learn tactics with uncertainty.
	•	A rich action space covering movement, aiming, shooting, abilities, and even buying and team communication, allowing the AI to do everything a player can do.
	•	A training regime leveraging self-play, curriculum learning, and massive parallel simulation to gradually teach the AI from basic combat to full strategic play, inspired by successful examples in research￼.
	•	A novel reward function with personality parameters, enabling the AI to adopt different play styles (aggressive fragger, strategic support, etc.) by simply tweaking reward weights￼. This makes our AI unique in its ability to produce diverse, human-like personalities.
	•	Strategies for difficulty scaling, so the AI can serve both as a friendly training partner for new players and a formidable opponent for veterans – achieved through controlled reaction times, aim accuracy, and using different checkpoints of the model.
	•	Consideration of exploration techniques and continuous improvement to ensure the AI finds creative strategies and doesn’t stagnate, as well as a plan for ongoing training updates and maintenance post-launch.
	•	A commitment to a unified model approach for all agents to maximize learning efficiency and coordination, with the flexibility to adjust behavior via inputs rather than maintaining many separate AIs￼.

By following this design, we expect to develop AI players that push the envelope of game AI: they will coordinate like a high-tier esports team, react and aim with impressive skill (yet configurable to be fair), use abilities intelligently, and adapt their strategy on the fly. The system will be robust and extensible, allowing us to improve it over time and tailor bot personalities to different game scenarios.

The next steps will involve prototyping the environment and model training on a smaller scale to validate these concepts. We will incrementally build up the complexity, monitor the training metrics discussed, and refine the parameters. Close collaboration between the AI engineers and game designers will be maintained to ensure the behaviors align with fun and balanced gameplay.

In summary, this document serves as our comprehensive roadmap to delivering a cutting-edge AI for our Valorant-style shooter. By adhering to these plans and iterating with data-driven insights, we aim to create an AI system that not only meets the technical objectives but also elevates the player experience, setting our game apart with AI opponents and teammates that feel remarkably lifelike, challenging, and engaging.